"","X","Title","Author","Subject","Abstract","Meta"
"1",1,"spline trajectory tracking and obstacle avoidance for mobile agents via convex optimization","akua dickson, christos g. cassandras, roberto tron","systems and control (eess.sy)","we propose an output feedback control-based motion planning technique for agents to enable them to converge to a specified polynomial trajectory while imposing a set of safety constraints on our controller to avoid collisions within the free configuration space (polygonal environment). to achieve this, we ) decompose our polygonal environment into different overlapping cells ) write out our polynomial trajectories as the output of a reference dynamical system with given initial conditions ) formulate convergence and safety constraints as linear matrix inequalities (lmis) on our controller using control lyapunov functions (clfs) and control barrier functions (cbfs) and ) solve a semi-definite programming (sdp) problem with convergence and safety constraints imposed to synthesize a controller for each convex cell. extensive simulations are included to test our motion planning method under different initial conditions and different reference trajectories. the synthesized controller is robust to changes in initial conditions and is always safe relative to the boundaries of the polygonal environment.",2024-03-25
"2",2,"bi-objective optimization in role mining","jason crampton, eduard eiben, gregory gutin, daniel karapetyan, diptapriyo majumdar","cryptography and security (cs.cr)","role mining is a technique used to derive a role-based authorization policy from an existing policy. given a set of users $u$, a set of permissions $p$ and a user-permission authorization relation $\mahtit{upa}\subseteq u\times p$, a role mining algorithm seeks to compute a set of roles $r$, a user-role authorization relation $\mathit{ua}\subseteq u\times r$ and a permission-role authorization relation $\mathit{pa}\subseteq r\times p$, such that the composition of $\mathit{ua}$ and $\mathit{pa}$ is close (in some appropriate sense) to $\mathit{upa}$.
in this paper, we first introduce the generalized noise role mining problem (gnrm) -- a generalization of the minnoise role mining problem -- which we believe has considerable practical relevance. extending work of fomin et al., we show that gnrm is fixed parameter tractable, with parameter $r + k$, where $r$ is the number of roles in the solution and $k$ is the number of discrepancies between $\mathit{upa}$ and the relation defined by the composition of $\mathit{ua}$ and $\mathit{pa}$. we further introduce a bi-objective optimization variant of gnrm, where we wish to minimize both $r$ and $k$ subject to upper bounds $r\le \bar{r}$ and $k\le \bar{k}$, where $\bar{r}$ and $\bar{k}$ are constants. we show that the pareto front of this bi-objective optimization problem (bo-gnrm) can be computed in fixed-parameter tractable time with parameter $\bar{r}+\bar{k}$.
we then report the results of our experimental work using the integer programming solver gurobi to solve instances of bo-gnrm. our key findings are that (a) we obtained strong support that gurobi's performance is fixed-parameter tractable, (b) our results suggest that our techniques may be useful for role mining in practice, based on our experiments in the context of three well-known real-world authorization policies.",2024-03-25
"3",3,"power-aware sparse reflect beamforming in active ris-aided interference channels","ruizhe long, hu zhou, ying-chang liang","information theory (cs.it)","active reconfigurable intelligent surface (ris) has attracted significant attention in wireless communications, due to its reflecting elements (res) capable of reflecting incident signals with not only phase shifts but also amplitude amplifications. in this paper, we are interested in active ris-aided interference channels in which $k$ user pairs share the same time and frequency resources with the aid of active ris. thanks to the promising amplitude amplification capability, activating a moderate number of res, rather than all of them, is sufficient for the active ris to mitigate cross-channel interferences. motivated by this, we propose a power-aware sparse reflect beamforming design for the active ris-aided interference channels, which allows the active ris to flexibly adjust the number of activated res for the sake of reducing hardware and power costs. specifically, we establish the power consumption model in which only those activated res consume the biasing and operation power that supports the amplitude amplification, yielding an $\ell_$-norm power consumption function. based on the proposed model, we investigate a sum-rate maximization problem and an active ris power minimization problem by carefully designing the sparse reflect beamforming vector. to solve these problems, we first replace the nonconvex $\ell_$-norm function with an iterative reweighted $\ell_$-norm function. then, fractional programming is used to solve the sum-rate maximization, while semidefinite programming together with the difference-of-convex algorithm (dca) is used to solve the active ris power minimization. numerical results show that the proposed sparse designs can notably increase the sum rate of user pairs and decrease the power consumption of active ris in interference channels.",2024-03-25
"4",4,"a monte carlo simulation of the broad band x-ray emission of the accreting millisecond x-ray pulsar maxi j-","yuan you (), shuang-nan zhang (), zhaosheng li (), mingyu ge () (() key laboratory of particle astrophysics, institute of high energy physics, chinese academy of sciences, beijing, china, () key laboratory of stars and interstellar medium, xiangtan university, xiangtan university, xiangtan, hunan, china)","high energy astrophysical phenomena (astro-ph.he)","maxi j- is an accreting millisecond x-ray pulsar (amxp) discovered in . according to the insight-hxmt data, the pulsations of this source extend all the way to over  kev, and its pulse profiles change from a single peak in low-energy range to double peaks in high-energy range. in this work, we simulate its energy spectra and pulse profiles with a compton scattering monte carlo program. the simulation results suggest that the low energy x-ray source on the neutron star surface should be pencil-beamed radiations from the magnetic poles, and there should be a boundary layer in a hollow cylinder shape between the accretion disc and the neutron star surface: the up-scattering of the polar radiations in the boundary layer leads to the double peak structure of the high-energy pulse profile. under this boundary layer geometry, we suggest that the rarity of amxps can be caused by the smearing of the boundary layer. to estimate the mass m and radius r of accretion-powered millisecond pulsars whose surface radiations are badly polluted by the accretion disk and boundary layer, the impact of compton scattering in the boundary layer on the radiation should be removed before employing the x-ray pulse profile modeling method.",2024-03-25
"5",5,"percentile optimization in wireless networks- part i: power control for max-min-rate to sum-rate maximization (and everything in between)","ahmad ali khan, raviraj adve","information theory (cs.it)","improving throughput for cell-edge users through coordinated resource allocation has been a long-standing driver of research in wireless cellular networks. while a variety of wireless resource management problems focus on sum utility, max-min utility and proportional fair utility, these formulations do not explicitly cater to cell-edge users and can, in fact, be disadvantageous to them. in this two-part paper series, we introduce a new class of optimization problems called percentile programs, which allow us to explicitly formulate problems that target lower-percentile throughput optimization for cell-edge users. part i focuses on the class of least-percentile throughput maximization through power control. this class subsumes the well-known max-min and max-sum-rate optimization problems as special cases. apart from these two extremes, we show that least-percentile rate programs are non-convex, non-smooth and strongly np-hard in general for multiuser interference networks, making optimization extremely challenging. we propose cyclic maximization algorithms that transform the original problems into equivalent block-concave forms, thereby enabling guaranteed convergence to stationary points. comparisons with state-of-the-art optimization algorithms such as successive convex approximation and sequential quadratic programming reveal that our proposed algorithms achieve superior performance while computing solutions orders of magnitude faster.",2024-03-25
"6",6,"safe reinforcement learning for constrained markov decision processes with stochastic stopping time","abhijit mazumdar, rafal wisniewski, manuela l. bujorianu","machine learning (cs.lg)","in this paper, we present an online reinforcement learning algorithm for constrained markov decision processes with a safety constraint. despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. to this end, we propose an algorithm based on linear programming that does not require a process model. we show that the learned policy is safe with high confidence. we also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. finally, we provide simulation results to show the efficacy of the proposed algorithm. further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.",2024-03-23
"7",7,"operational experience and r&d results using the google cloud for high energy physics in the atlas experiment","fernando barreiro megino, kaushik de, johannes elmsheuser, alexei klimentov, mario lassnig, miles euell, nikolai hartmann, tadashi maeno, verena martinez outschoorn, jay ajitbhai sandesara, dustin sell","high energy physics - experiment (hep-ex)","the atlas experiment at cern relies on a worldwide distributed computing grid infrastructure to support its physics program at the large hadron collider. atlas has integrated cloud computing resources to complement its grid infrastructure and conducted an r&d program on google cloud platform. these initiatives leverage key features of commercial cloud providers: lightweight configuration and operation, elasticity and availability of diverse infrastructure. this paper examines the seamless integration of cloud computing services as a conventional grid site within the atlas workflow management and data management systems, while also offering new setups for interactive, parallel analysis. it underscores pivotal results that enhance the on-site computing model and outlines several r&d projects that have benefited from large-scale, elastic resource provisioning models. furthermore, this study discusses the impact of cloud-enabled r\&d projects in three domains: accelerators and ai/ml, arm cpus and columnar data analysis techniques.",2024-03-23
"8",8,"energy efficient design of active star-ris-aided swipt systems","sajad faramarzi, hosein zarini, sepideh javadi, mohammad robat mili, rui zhang, george k. karagiannidis, naofal al-dhahir","information theory (cs.it)","in this paper, we consider the downlink transmission of a multi-antenna base station (bs) supported by an active simultaneously transmitting and reconfigurable intelligent surface (star-ris) to serve single-antenna users via simultaneous wireless information and power transfer (swipt). in this context, we formulate an energy efficiency maximisation problem that jointly optimises the gain, element selection and phase shift matrices of the active star-ris, the transmit beamforming of the bs and the power splitting ratio of the users. with respect to the highly coupled and non-convex form of this problem, an alternating optimisation solution approach is proposed, using tools from convex optimisation and reinforcement learning. specifically, semi-definite relaxation (sdr), difference of concave functions (dc), and fractional programming techniques are employed to transform the non-convex optimisation problem into a convex form for optimising the bs beamforming vector and the power splitting ratio of the swipt. then, by integrating meta-learning with the modified deep deterministic policy gradient (ddpg) and soft actor-critical (sac) methods, a combinatorial reinforcement learning network is developed to optimise the element selection, gain and phase shift matrices of the active star-ris. our simulations show the effectiveness of the proposed resource allocation scheme. furthermore, our proposed active star-ris-based swipt system outperforms its passive counterpart by % on average.",2024-03-23
"9",9,"codeshell technical report","rui xie, zhengran zeng, zhuohao yu, chang gao, shikun zhang, wei ye","software engineering (cs.se)","code large language models mark a pivotal breakthrough in artificial intelligence. they are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. in this technical report, we present codeshell-base, a seven billion-parameter foundation model with k context length, showcasing exceptional proficiency in code comprehension. by incorporating grouped-query attention and rotary positional embedding into gpt-, codeshell-base integrates the structural merits of starcoder and codellama and forms its unique architectural design. we then carefully built a comprehensive data pre-processing process, including similar data deduplication, perplexity-based data filtering, and model-based data filtering. through this process, we have curated  billion high-quality pre-training data from github. benefiting from the high-quality data, codeshell-base outperforms codellama in humaneval after training on just  billion tokens ( epochs). we have conducted extensive experiments across multiple language datasets, including python, java, and c++, and the results indicate that our model possesses robust foundational capabilities in code comprehension and generation.",2024-03-23
"10",10,"ac: algebraic computation checker for circuit constraints in zkps","hao chen, minyu chen, ruibang liu, guoqiang li","software engineering (cs.se)","zkp systems have surged attention and held a fundamental role in contemporary cryptography. zk-snark protocols dominate the zkp usage, often implemented through arithmetic circuit programming paradigm. however, underconstrained or overconstrained circuits may lead to bugs. underconstrained circuits refer to circuits that lack the necessary constraints, resulting in unexpected solutions in the circuit and causing the verifier to accept a bogus witness. overconstrained circuits refer to circuits that are constrained excessively, resulting in the circuit lacking necessary solutions and causing the verifier to accept no witness, rendering the circuit meaningless. this paper introduces a novel approach for pinpointing two distinct types of bugs in zkp circuits. the method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving polynomial equation systems over a finite field by algebraic computation. the classification of verification results is refined, greatly enhancing the expressive power of the system. we proposed a tool, ac, to represent the implementation of this method. experiments demonstrate that ac represents a substantial % increase in the checked ratio compared to prior work. within a solvable range, the checking time of ac has also exhibited noticeable improvement, demonstrating a magnitude increase compared to previous efforts.",2024-03-23
"11",11,"(towards a) statistical probabilistic lazy lambda calculus","radha jagadeesan","logic in computer science (cs.lo)","we study the desiderata on a model for statistical probabilistic programming languages. we argue that they can be met by a combination of traditional tools, namely open bisimulation and probabilistic simulation.",2024-03-22
"12",12,"programmers prefer individually assigned tasks vs. shared responsibility","adela krylova, roman makarov, sergei pasynkov, yegor bugayenko","software engineering (cs.se)","in traditional management, tasks are typically assigned to individuals, with each worker taking full responsibility for the success or failure of a task. in contrast, modern agile, lean, and extreme programming practices advocate for shared responsibility, where an entire group is accountable for the outcome of a project or task. despite numerous studies in other domains, the preferences of programmers have not been thoroughly analyzed. to address this gap, we conducted a survey featuring seven situational questions and collected the opinions of  software development practitioners. our findings reveal that programmers prefer tasks to be assigned to them on an individual basis and appreciate taking personal responsibility for failures, as well as receiving individual rewards for successes. understanding these preferences is crucial for project managers aiming to optimize team dynamics and ensure the successful completion of software projects.",2024-03-22
"13",13,"real-time safety index adaptation for parameter-varying systems via determinant gradient ascend","rui chen, weiye zhao, ruixuan liu, weiyang zhang, changliu liu","systems and control (eess.sy)","safety index synthesis (sis) is critical for deriving safe control laws. recent works propose to synthesize a safety index (si) via nonlinear programming and derive a safe control law such that the system ) achieves forward invariant (fi) with some safe set and ) guarantees finite time convergence (ftc) to that safe set. however, real-world system dynamics can vary during run-time, making the control law infeasible and invalidating the initial si. since the full sis nonlinear programming is computationally expensive, it is infeasible to re-synthesize the si each time the dynamics are perturbed. to address that, this paper proposes an efficient approach to adapting the si to varying system dynamics and maintaining the feasibility of the safe control law. the proposed method leverages determinant gradient ascend and derives a closed-form update to safety index parameters, enabling real-time adaptation performance. a numerical study validates the effectiveness of our approach.",2024-03-22
"14",14,"fat api bindings of c++ objects into scripting languages","russell k. standish","programming languages (cs.pl)","a fat api exposes nearly all of a c++ object's public attributes and methods to a consuming environment, such as a scripting language, or web client. this can be contrasted with a conventional, or thin api, where the api is defined up front, and the c++ object provides the implementation, most of which is private to the c++ layer. obviously, reflection is required to expose c++ objects to a consuming layer like this -- this paper explores using the classdesc system to implement reflection of c++ objects into a javascript/typescript environment via a restservice, and also via a node.js api module.",2024-03-22
"15",15,"a survey of neural code intelligence: paradigms, advances and beyond","qiushi sun, zhirui chen, fangzhi xu, kanzhi cheng, chang ma, zhangyue yin, jianing wang, chengcheng han, renyu zhu, shuai yuan, qipeng guo, xipeng qiu, pengcheng yin, xiaoli li, fei yuan, lingpeng kong, xiang li, zhiyong wu","software engineering (cs.se)","neural code intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. bridging the gap between natural language and programming language, this domain has drawn significant attention from researchers in both research communities over the past few years. this survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over  representative models and their variants, more than  categories of tasks, and an extensive coverage of over  related works. we follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of large language models). concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. for applications, we also observe a co-evolving shift. it spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. an ongoing, dynamically updated project and resources associated with this survey have been released at this https url.",2024-03-21
"16",16,"personalized programming guidance based on deep programming learning style capturing","yingfan liu, renyu zhu, ming gao","computers and society (cs.cy)","with the rapid development of big data and ai technology, programming is in high demand and has become an essential skill for students. meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. however, two significant challenges still need to be addressed in programming: c) how to recognize complex programming behaviors; c) how to capture intrinsic learning patterns that align with the actual learning process. to fill these gaps, in this paper, we propose a novel model called programming exercise recommender with learning style (pers), which simulates learners' intricate programming behaviors. specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to capture the changes of consecutive code submissions (which addresses c). to better profile programming behaviors, we extend the felder-silverman learning style model, a classical pedagogical theory, to perceive intrinsic programming patterns. based on this, we align three latent vectors to record and update programming ability, processing style, and understanding style, respectively (which addresses c). we perform extensive experiments on two real-world datasets to verify the rationality of modeling programming learning styles and the effectiveness of pers for personalized programming guidance.",2024-02-20
"17",17,"designing robust linear output feedback controller based on clf-cbf framework via linear~programming(lp-clf-cbf)","mahroo bahreinian, mehdi kermanshah, roberto tron","systems and control (eess.sy)","we consider the problem of designing output feedback controllers that use measurements from a set of landmarks to navigate through a cell-decomposable environment using duality, control lyapunov and barrier functions (clf, cbf), and linear programming. we propose two objectives for navigating in an environment, one to traverse the environment by making loops and one by converging to a stabilization point while smoothing the transition between consecutive cells. we test our algorithms in a simulation environment, evaluating the robustness of the approach to practical conditions, such as bearing-only measurements, and measurements acquired with a camera with a limited field of view.",2024-03-21
"18",18,"early planet formation in embedded disks (edisk) xiii: aligned disks with non-settled dust around the newly resolved class  protobinary r cra iras ","frankie j. encalada, leslie w. looney, shigehisa takakuwa, john j. tobin, nagayoshi ohashi, jes k. jørgensen, zhi-yun li, yuri aikawa, yusuke aso, patrick m. koch, woojin kwon, shih-ping lai, chang won lee, zhe-yu daniel lin, alejandro santamarıa-miranda, itziar de gregorio-monsalvo, nguyen thi phuong, adele plunkett, jinshi sai (insa choi), rajeeb sharma, hsi-wei yen, ilseung han","solar and stellar astrophysics (astro-ph.sr)","young protostellar binary systems, with expected ages less than $\sim$$^$ years, are little modified since birth, providing key clues to binary formation and evolution. we present a first look at the young, class  binary protostellar system r cra iras  from the early planet formation in embedded disks (edisk) alma large program, which observed the system in the . mm continuum emission, $^{}$co (-), $^{}$co (-), c$^{}$o (-), so ($_$-$_$), and nine other molecular lines that trace disk, envelope, shocks, and outflows. with a continuum resolution of $\sim$.$^{\prime\prime}$ ($\sim$ au, at a distance of  pc), we characterize the newly discovered binary system with a separation of  au, their circumstellar disks, and a circumbinary disk-like structure. the circumstellar disk radii are .$\pm$. and .$\pm$. au for sources a and b, respectively, and their circumstellar disk dust masses are estimated as .$\pm$. and .$\pm$. m$_{\earth}$. the circumstellar disks and the circumbinary structure have well aligned position angles and inclinations, indicating formation in a smooth, ordered process such as disk fragmentation. in addition, the circumstellar disks have a near/far-side asymmetry in the continuum emission suggesting that the dust has yet to settle into a thin layer near the midplane. spectral analysis of co isotopologues reveals outflows that originate from both of the sources and possibly from the circumbinary disk-like structure. furthermore, we detect keplerian rotation in the $^{}$co isotopologues toward both circumstellar disks and likely keplerian rotation in the circumbinary structure; the latter suggests that it is probably a circumbinary disk.",2024-03-21
"19",19,"can chatgpt detect deepfakes? a study of using multimodal large language models for media forensics","shan jia, reilin lyu, kangran zhao, yize chen, zhiyuan yan, yan ju, chuanbo hu, xin li, baoyuan wu, siwei lyu","artificial intelligence (cs.ai)","deepfakes, which refer to ai-generated media content, have become an increasing concern due to their use as a means for disinformation. detecting deepfakes is currently solved with programmed machine learning algorithms. in this work, we investigate the capabilities of multimodal large language models (llms) in deepfake detection. we conducted qualitative and quantitative experiments to demonstrate multimodal llms and show that they can expose ai-generated images through careful experimental design and prompt engineering. this is interesting, considering that llms are not inherently tailored for media forensic tasks, and the process does not require programming. we discuss the limitations of multimodal llms for these tasks and suggest possible improvements.",2024-03-21
"20",20,"real groups, symmetric varieties and langlands duality","tsao-hsien chen, david nadler","representation theory (math.rt)","let $g_\mathbb r$ be a connected real reductive group and let $x$ be the corresponding complex symmetric variety under the cartan bijection. we construct a canonical equivalence between the relative satake category of $g(\mathcal o)$-equivariant $\mathbb c$-constructible complexes on the loop space of $x$ and the real satake category of $g_\mathbb r(\mathcal o_\mathbb r)$-equivariant $\mathbb c$-constructible complexes on the real affine grassmannian. we show that the equivalence is $t$-exact with respect to the natural perverse $t$-structures and is compatible with the fusion products and hecke actions. we further show that the relative satake category is equivalent to the category of $\mathbb c$-constructible complexes on the moduli stack of $g_\mathbb r$-bundles on the real projective line $\mathbb p^(\mathbb r)$ and hence provides a connection between the relative langlands program and the geometric langlands program for real groups. we provide numerous applications of the main theorems to real and relative langlands duality including the formality and commutativity conjectures for the real and relative satake categories and an identification of the dual groups for $g_\mathbb r$ and $x$.",2024-03-20
"21",21,"depyf: open the opaque box of pytorch compiler for machine learning researchers","kaichao you, runsheng bai, meng cao, jianmin wang, ion stoica, mingsheng long","machine learning (cs.lg)","pytorch \texttt{.x} introduces a compiler designed to accelerate deep learning programs. however, for machine learning researchers, adapting to the pytorch compiler to full potential can be challenging. the compiler operates at the python bytecode level, making it appear as an opaque box. to address this, we introduce \texttt{depyf}, a tool designed to demystify the inner workings of the pytorch compiler. \texttt{depyf} decompiles bytecode generated by pytorch back into equivalent source code, and establishes connections between in-memory code objects and their on-disk source code counterparts. this feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. notably, \texttt{depyf} is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. the project is \href{this https url}{ openly available} and is recognized as a \href{this https url}{pytorch ecosystem project}.",2024-03-14
"22",22,"taming differentiable logics with coq formalisation","reynald affeldt, alessandro bruni, ekaterina komendantskaya, natalia ślusarz, kathrin stark","logic in computer science (cs.lo)","for performance and verification in machine learning, new methods have recently been proposed that optimise learning systems to satisfy formally expressed logical properties. among these methods, differentiable logics (dls) are used to translate propositional or first-order formulae into loss functions deployed for optimisation in machine learning. at the same time, recent attempts to give programming language support for verification of neural networks showed that dls can be used to compile verification properties to machine-learning backends. this situation is calling for stronger guarantees about the soundness of such compilers, the soundness and compositionality of dls, and the differentiability and performance of the resulting loss functions. in this paper, we propose an approach to formalise existing dls using the mathematical components library in the coq proof assistant. thanks to this formalisation, we are able to give uniform semantics to otherwise disparate dls, give formal proofs to existing informal arguments, find errors in previous work, and provide formal proofs to missing conjectured properties. this work is meant as a stepping stone for the development of programming language support for verification of machine learning.",2024-03-20
"23",23,"tensor quantum programming","a. termanova, ar. melnikov, e. mamenchikov, n. belokonev, s. dolgov, a. berezutskii, r. ellerbrock, c. mansell, m. perelshtein","quantum physics (quant-ph)","running quantum algorithms often involves implementing complex quantum circuits with such a large number of multi-qubit gates that the challenge of tackling practical applications appears daunting. to date, no experiments have successfully demonstrated a quantum advantage due to the ease with which the results can be adequately replicated on classical computers through the use of tensor network algorithms. additionally, it remains unclear even in theory where exactly these advantages are rooted within quantum systems because the logarithmic complexity commonly associated with quantum algorithms is also present in algorithms based on tensor networks. in this article, we propose a novel approach called tensor quantum programming, which leverages tensor networks for hybrid quantum computing. our key insight is that the primary challenge of algorithms based on tensor networks lies in their high ranks (bond dimensions). quantum computing offers a potential solution to this challenge, as an ideal quantum computer can represent tensors with arbitrarily high ranks in contrast to classical counterparts, which indicates the way towards quantum advantage. while tensor-based vector-encoding and state-readout are known procedures, the matrix-encoding required for performing matrix-vector multiplications directly on quantum devices remained unsolved. here, we developed an algorithm that encodes matrix product operators into quantum circuits with a depth that depends linearly on the number of qubits. it demonstrates effectiveness on up to  qubits for several matrices frequently encountered in differential equations, optimization problems, and quantum chemistry. we view this work as an initial stride towards the creation of genuinely practical quantum algorithms.",2024-03-20
"24",24,"regent based parallel meshfree lskum solver for heterogenous hpc platforms","sanath salil, nischay ram mamidi, anil nemili, elliott slaughter","distributed, parallel, and cluster computing (cs.dc)","regent is an implicitly parallel programming language that allows the development of a single codebase for heterogeneous platforms targeting cpus and gpus. this paper presents the development of a parallel meshfree solver in regent for two-dimensional inviscid compressible flows. the meshfree solver is based on the least squares kinetic upwind method. example codes are presented to show the difference between the regent and cuda-c implementations of the meshfree solver on a gpu node. for cpu parallel computations, details are presented on how the data communication and synchronisation are handled by regent and fortran+mpi codes. the regent solver is verified by applying it to the standard test cases for inviscid flows. benchmark simulations are performed on coarse to very fine point distributions to assess the solver's performance. the computational efficiency of the regent solver on an a gpu is compared with an equivalent meshfree solver written in cuda-c. the codes are then profiled to investigate the differences in their performance. the performance of the regent solver on cpu cores is compared with an equivalent explicitly parallel fortran meshfree solver based on mpi. scalability results are shown to offer insights into performance.",2024-03-20
"25",25,"c analyzer : a static program analysis tool for c programs","rajendra kumar solanki","programming languages (cs.pl)","in our times, when the world is increasingly becoming more dependent on software programs, writing bug-free, correct programs is crucial. program verification based on formal methods can guarantee this by detecting run-time errors in safety-critical systems to avoid possible adverse impacts on human life and save time and money.
this project work tries to leverage abstract interpretation techniques for static analysis of c programs. c analyzer is a tool developed for static analysis of c programs. this implementation of c analyzer provides a plug-and-play domain architecture for multiple abstract domains to be used. c analyzer supports four abstract domains - interval, octagon, polyhedra, and bit vector. we use these different domains for required precision in program verification. c analyzer tool uses llvm c/c++ compiler frontend clang api to generate and traverse the control flow graph (cfg) of a given c program. this tool generates invariants in different abstract domains for statements in basic blocks of cfg during cfg traversal. using these invariants, some properties of a program, such as dividing by zero, modulus zero, arithmetic overflow, etc., can be analyzed. we also use a source-to-source transformation tool, cil (common intermediate language), to transform some c constructs into simpler constructs, such as transforming logical operators, switch statements, and conditional operators into if-else ladders and transforming do-while and for loops into while loops.
using c analyzer, c program constructs such as declarations, assignments, binary operations (arithmetic, relational, bitwise shift, etc.), conditions (if-else), loops (while, do while, for loop), nested conditions, and nested loops can be analyzed. currently, this tool does not support arrays, structures, unions, pointers, or function calls.",2024-01-28
"26",26,"navigating compiler errors with ai assistance -- a study of gpt hints in an introductory programming course","maciej pankiewicz, ryan s. baker","software engineering (cs.se)","we examined the efficacy of ai-assisted learning in an introductory programming course at the university level by using a gpt- model to generate personalized hints for compiler errors within a platform for automated assessment of programming assignments. the control group had no access to gpt hints. in the experimental condition gpt hints were provided when a compiler error was detected, for the first half of the problems in each module. for the latter half of the module, hints were disabled. students highly rated the usefulness of gpt hints. in affect surveys, the experimental group reported significantly higher levels of focus and lower levels of confrustion (confusion and frustration) than the control group. for the six most commonly occurring error types we observed mixed results in terms of performance when access to gpt hints was enabled for the experimental group. however, in the absence of gpt hints, the experimental group's performance surpassed the control group for five out of the six error types.",2024-03-19
"27",27,"prototipo de video juego activo basado en una cámara d para motivar la actividad física en niños y adultos mayores","benjamín ojeda magaña, josé guadalupe robledo hernández, leopoldo gómez barba, victor manuel rangel cobián","human-computer interaction (cs.hc)","this document describes the development of a video game prototype designed to encourage physical activity among children and older adults. the prototype consists of a laptop, a camera with d sensors, and optionally requires an lcd screen or a projector. the programming component of this prototype was developed in scratch, a programming language geared towards children, which greatly facilitates the creation of a game tailored to the users' preferences. the idea to create such a prototype originated from the desire to offer an option that promotes physical activity among children and adults, given that a lack of physical exercise is a predominant factor in the development of chronic degenerative diseases such as diabetes and hypertension, to name the most common. as a result of this initiative, an active video game prototype was successfully developed, based on a ping-pong game, which allows both children and adults to interact in a fun way while encouraging the performance of physical activities that can positively impact the users' health.",2024-03-19
"28",28,"ikspark: an inverse kinematics solver using semidefinite relaxation and rank minimization","liangting wu, roberto tron","robotics (cs.ro)","inverse kinematics (ik) is a fundamental problem frequently occurred in robot control and motion planning. however, the problem is nonconvex because the kinematic map between the configuration and task spaces is generally nonlinear, which makes it challenging for fast and accurate solutions. the problem can be more complicated with the existence of different physical constraints imposed by the robot structure. in this paper, we develop an inverse kinematics solver named ikspark (inverse kinematics using semidefinite programming and rank minimization) that can find solutions for robots with various structures, including open/closed kinematic chains, spherical, revolute, and/or prismatic joints. the solver works in the space of rotation matrices of the link reference frames and involves solving only convex semidefinite problems (sdps). specifically, the ik problem is formulated as an sdp with an additional rank- constraint on symmetric matrices with constant traces. the solver first solves this sdp disregarding the rank constraint to get a start point and then finds the rank- solution iteratively via a rank minimization algorithm with proven local convergence. compared to other work that performs sdp relaxation for ik problems, our formulation is simpler, and uses variables with smaller sizes. we validate our approach via simulations on different robots, comparing against a standard ik method.",2024-03-18
"29",29,"routing and scheduling in answer set programming applied to multi-agent path finding: preliminary report","roland kaminski, torsten schaub, tran cao son, jiří švancara, philipp wanko","artificial intelligence (cs.ai)","we present alternative approaches to routing and scheduling in answer set programming (asp), and explore them in the context of multi-agent path finding. the idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. this also abolishes the need for fixed upper bounds on the length of plans. the trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. while this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in asp in a feasible way. this is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. we formally elaborate upon this idea and present several resulting asp encodings. finally, we demonstrate their effectiveness via an empirical analysis.",2024-03-18
"30",30,"semidefinite programming on population clustering: a local analysis","shuheng zhou","statistics theory (math.st)","in this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $$ sub-gaussian distributions. in particular, we design and analyze two computational efficient algorithms to partition data into two groups approximately according to their population of origin given a small sample in a recent paper (zhou a). our work is motivated by the application of clustering individuals according to their population of origin using markers, when the divergence between any two of the populations is small. moreover, we are interested in the case that individual features are of low average quality $\gamma$, and we want to use as few of them as possible to correctly partition the sample. here we use $p \gamma$ to denote the $\ell_^$ distance between two population centers (mean vectors), namely, $\mu^{()}$, $\mu^{()}$ $\in$ ${\mathbb r}^p$. we allow a full range of tradeoffs between $n, p, \gamma$ in the sense that partial recovery (success rate $< \%$) is feasible once the signal to noise ratio $s^ := \min\{np \gamma^, p \gamma\}$ is lower bounded by a constant. our work builds upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features in zhou (a). more importantly, we prove that the misclassification error decays exponentially with respect to the snr $s^$ in the present paper. the significance of such an exponentially decaying error bound is: when $s^ =\omega(\log n)$, perfect recovery of the cluster structure is accomplished. this result was introduced in zhou (a) without a proof. we therefore present the full proof in the present work.",2023-11-23
"31",31,"beamforming design for semantic-bit coexisting communication system","maojun zhang, guangxu zhu, richeng jin, xiaoming chen, qingjiang shi, caijun zhong, kaibin huang","information theory (cs.it)","semantic communication (semcom) is emerging as a key technology for future sixth-generation (g) systems. unlike traditional bit-level communication (bitcom), semcom directly optimizes performance at the semantic level, leading to superior communication efficiency. nevertheless, the task-oriented nature of semcom renders it challenging to completely replace bitcom. consequently, it is desired to consider a semantic-bit coexisting communication system, where a base station (bs) serves semcom users (sem-users) and bitcom users (bit-users) simultaneously. such a system faces severe and heterogeneous inter-user interference. in this context, this paper provides a new semantic-bit coexisting communication framework and proposes a spatial beamforming scheme to accommodate both types of users. specifically, we consider maximizing the semantic rate for semantic users while ensuring the quality-of-service (qos) requirements for bit-users. due to the intractability of obtaining the exact closed-form expression of the semantic rate, a data driven method is first applied to attain an approximated expression via data fitting. with the resulting complex transcendental function, majorization minimization (mm) is adopted to convert the original formulated problem into a multiple-ratio problem, which allows fractional programming (fp) to be used to further transform the problem into an inhomogeneous quadratically constrained quadratic programs (qcqp) problem. solving the problem leads to a semi-closed form solution with undetermined lagrangian factors that can be updated by a fixed point algorithm. extensive simulation results demonstrate that the proposed beamforming scheme significantly outperforms conventional beamforming algorithms such as zero-forcing (zf), maximum ratio transmission (mrt), and weighted minimum mean-square error (wmmse).",2024-03-18
"32",32,"looper: a learned automatic code optimizer for polyhedral compilers","massinissa merouani, khaled afif boudaoud, iheb nassim aouadj, nassim tchoulak, islem kara bernou, hamza benyamina, fatima benbouzid-si tayeb, karima benatchba, hugh leather, riyadh baghdadi","programming languages (cs.pl)","while polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. this has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. state-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. while such a proof-of-concept has shown promise, it still has significant limitations. state-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. they also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. these limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach. in this paper, we introduce looper, the first polyhedral autoscheduler that uses a deep-learning based cost model and covers a large set of affine transformations and programs. it supports the exploration of a large set of affine transformations, allowing the application of complex sequences of polyhedral transformations. it also supports the optimization of programs with multiple loop nests and with rectangular and non-rectangular iteration domains, allowing the optimization of an extensive set of programs. we implement and evaluate looper and show that it achieves speedups over the state-of-the-art. on the polybench benchmark, looper achieves a geometric mean speedup of .x over tiramisu. looper also achieves competitive speedups with a geometric mean speedup of .x over pluto, a state-of-the-art polyhedral compiler that does not use a machine-learning based cost model.",2024-03-18
"33",33,"table-lookup mac: scalable processing of quantised neural networks in fpga soft logic","daniel gerlinghoff, benjamin chen ming choong, rick siow mong goh, weng-fai wong, tao luo","hardware architecture (cs.ar)","recent advancements in neural network quantisation have yielded remarkable outcomes, with three-bit networks reaching state-of-the-art full-precision accuracy in complex tasks. these achievements present valuable opportunities for accelerating neural networks by computing in reduced precision. implementing it on fpgas can take advantage of bit-level reconfigurability, which is not available on conventional cpus and gpus. simultaneously, the high data intensity of neural network processing has inspired computing-in-memory paradigms, including on fpga platforms. by programming the effects of trained model weights as lookup operations in soft logic, the transfer of weight data from memory units can be avoided, alleviating the memory bottleneck. however, previous methods face poor scalability - the high logic utilisation limiting them to small networks/sub-networks of binary models with low accuracy. in this paper, we introduce table lookup multiply-accumulate (tlmac) as a framework to compile and optimise quantised neural networks for scalable lookup-based processing. tlmac clusters and maps unique groups of weights to lookup-based processing elements, enabling highly parallel computation while taking advantage of parameter redundancy. further place and route algorithms are proposed to reduce lut utilisation and routing congestion. we demonstrate that tlmac significantly improves the scalability of previous related works. our efficient logic mapping and high degree of reuse enables entire imagenet-scale quantised models with full-precision accuracy to be implemented using lookup-based computing on one commercially available fpga.",2024-03-18
"34",34,"multiscale quantile regression with local error control","zhi liu, housen li","methodology (stat.me)","for robust and efficient detection of change points, we introduce a novel methodology muscle (multiscale quantile segmentation controlling local error) that partitions serial data into multiple segments, each sharing a common quantile. it leverages multiple tests for quantile changes over different scales and locations, and variational estimation. unlike the often adopted global error control, muscle focuses on local errors defined on individual segments, significantly improving detection power in finding change points. meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter. further, we obtain the consistency and the localisation error rates in estimating change points, under mild signal-to-noise-ratio conditions. both match (up to log factors) the minimax optimality results in the gaussian setup. all theories hold under the only distributional assumption of serial independence. incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing muscle. extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness. an implementation via r package muscle is available from github.",2024-03-17
"35",35,"frequency-reactive power optimization strategy of grid-forming offshore wind farm using dru-hvdc transmission","zhekai li, kun han, xu cai, renxin yang, haotian yu, kepeng xia, lulu liu","optimization and control (math.oc)","the diode rectifier unit-based high voltage direct current (dru-hvdc) transmission with grid-forming (gfm) wind turbine is becoming a promising scheme for offshore wind farm(owf) integration due to its high reliability and low cost. in this scheme, the ac network of the owf and the dru has completely different synchronization mechanisms and power flow characteristics from the traditional power system. to optimize the power flow and reduce the net loss, this paper carries out the power flow modeling and optimization analysis for the dru-hvdc transmission system with grid-forming owfs. the influence of the dru and the gfm wind turbines on the power flow of the system is analyzed. on this basis, improved constraint conditions are proposed and an optimal power flow (opf) method is established. this method can minimize the power loss by adjusting the reactive power output of each wind turbine and internal network frequency. finally, based on matlab, this paper uses yalmip toolkit and cplex mathematical solver to realize the programming solution of the opf model proposed in this paper. the results show that the proposed optimization strategy can effectively reduce the power loss of the entire owf and the transmission system with an optimization ratio of network losses exceeding .%.",2024-03-16
"36",36,"mitigation and optimization of induced seismicity using physics-based forecasting","ryley g hill, matthew weingarten, cornelius langenbruch, yuri fialko","geophysics (physics.geo-ph)","fluid injection can induce seismicity by altering stresses on pre-existing faults. here, we investigate minimizing induced seismic hazard by optimizing injection operations in a physics-based forecasting framework. we built a d finite element model of the poroelastic crust for the raton basin, central us, and used it to estimate time dependent coulomb stress changes due to ~ years of wastewater injection in the region. our finite element model is complemented by a statistical analysis of the seismogenic index (si), a proxy for critically stressed faults affected by variations in the pore pressure. forecasts of seismicity rate from our hybrid physics-based statistical model suggest that induced seismicity in the raton basin, from  - , is still driven by wastewater injection. our model suggests that pore pressure diffusion is the dominant cause of coulomb stress changes at seismogenic depth, with poroelastic stress changes contributing about % to the driving force. linear programming optimization for the raton basin reveals that it is feasible to reduce seismic hazard for a given amount of injected fluid (safety objective) or maximize fluid injection for a prescribed seismic hazard (economic objective). the optimization tends to spread out high-rate injectors and shift them to regions of lower si. the framework has practical importance as a tool to manage injection rate per unit field area to reduce induced seismic hazard. our optimization framework is both flexible and adaptable to mitigate induced seismic hazard in other regions and for other types of subsurface fluid injection.",2024-03-15
"37",37,"unsupervised threat hunting using continuous bag-of-terms-and-time (cbott)","varol kayhan, shivendu shivendu, rouzbeh behnia, clinton daniel, manish agrawal","cryptography and security (cs.cr)","threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures. it can be performed in several ways, one of which is based on detecting anomalies. we propose an unsupervised framework, called continuous bag-of-terms-and-time (cbott), and publish its application programming interface (api) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among siem logs geared toward process auditing on endpoint devices. analyses show that our framework consistently outperforms benchmark approaches. when logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between .-.) while benchmark approaches identify the same anomalies at lower percentiles (between .-.). this framework can be used by other researchers to conduct benchmark analyses and cybersecurity analysts to find anomalies in siem logs.",2024-03-15
"38",38,"lyznet: a lightweight python tool for learning and verifying neural lyapunov functions and regions of attraction","jun liu, yiming meng, maxwell fitzsimmons, ruikun zhou","systems and control (eess.sy)","in this paper, we describe a lightweight python framework that provides integrated learning and verification of neural lyapunov functions for stability analysis. the proposed tool, named lyznet, learns neural lyapunov functions using physics-informed neural networks (pinns) to solve zubov's equation and verifies them using satisfiability modulo theories (smt) solvers. what distinguishes this tool from others in the literature is its ability to provide verified regions of attraction close to the domain of attraction. this is achieved by encoding zubov's partial differential equation (pde) into the pinn approach. by embracing the non-convex nature of the underlying optimization problems, we demonstrate that in cases where convex optimization, such as semidefinite programming, fails to capture the domain of attraction, our neural network framework proves more successful. the tool also offers automatic decomposition of coupled nonlinear systems into a network of low-dimensional subsystems for compositional verification. we illustrate the tool's usage and effectiveness with several numerical examples, including both non-trivial low-dimensional nonlinear systems and high-dimensional systems. the repository of the tool can be found at this https url.",2024-03-15
"39",39,"compositionally verifiable vector neural lyapunov functions for stability analysis of interconnected nonlinear systems","jun liu, yiming meng, maxwell fitzsimmons, ruikun zhou","systems and control (eess.sy)","while there has been increasing interest in using neural networks to compute lyapunov functions, verifying that these functions satisfy the lyapunov conditions and certifying stability regions remain challenging due to the curse of dimensionality. in this paper, we demonstrate that by leveraging the compositional structure of interconnected nonlinear systems, it is possible to verify neural lyapunov functions for high-dimensional systems beyond the capabilities of current satisfiability modulo theories (smt) solvers using a monolithic approach. our numerical examples employ neural lyapunov functions trained by solving zubov's partial differential equation (pde), which characterizes the domain of attraction for individual subsystems. these examples show a performance advantage over sums-of-squares (sos) polynomial lyapunov functions derived from semidefinite programming.",2024-03-15
"40",40,"one-shot learning for mips with sos constraints","charly robinson la rocca, jean-françois cordeau, emma frejinger","optimization and control (math.oc)","efficient algorithms and solvers are required to provide optimal or near-optimal solutions quickly and enable organizations to react promptly to dynamic situations such as supply chain disruptions or changing customer demands. state-of-the-art mixed-integer programming (mip) solvers are crafted to tackle a wide variety of problems, yet many real-world situations are characterized by problem instances that originate from a narrow distribution. this has inspired the creation of tailored approaches that exploit historical data to inform heuristic design. deep learning (dl) methods are typically used in this context to extract patterns from data, but they require large datasets and comprehensive hyperparameter tuning for strong performance. this article describes a one-shot learning heuristic that leverages solutions discovered within the branch-and-bound tree to construct a model with minimal overhead. we evaluate our method on the locomotive assignment problem (lap) and sets of miplib instances that contain constraints based on special ordered sets of type . experimental results include a comparison with multiple primal heuristics and state-of-the-art mip solvers. we show that the method is most effective with cplex in terms of the average primal gap.",2024-03-14
"41",41,"teaching machines to code: smart contract translation with llms","rabimba karanjai, lei xu, weidong shi","software engineering (cs.se)","the advent of large language models (llms) has marked a significant milestone in the realm of artificial intelligence, with their capabilities often matching or surpassing human expertise in various domains. among these achievements, their adeptness in translation tasks stands out, closely mimicking the intricate and preliminary processes undertaken by human translators to ensure the fidelity and quality of the translated content. despite the advancements in utilizing llms for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the llm, remains largely unexplored. in our research, we present a pioneering approach, solmover, which harnesses the synergy of two distinct llms within a unified framework. this framework is designed to grasp coding principles and apply this understanding to the translation of code into an unfamiliar language. our study delves into the capacity of llms to mimic human learning processes, offering an in-depth evaluation of our methodology for converting smart contracts written in solidity to move, a language with limited resources. the framework employs one llm to decipher coding conventions for the new language, creating a blueprint for the second llm, which, lacking planning abilities, possesses coding expertise. the empirical evidence from our experiments suggests that solmover substantially enhances performance compared to gpt-.-turbo-, and achieves superior results over competitors such as palm and mixtral-xb-instruct. additionally, our analysis highlights the efficacy of our bug mitigation strategy in elevating code quality across all models, even outside the solmover framework.",2024-03-13
"42",42,"corais: lightweight real-time scheduler for multi-edge cooperative computing","yujiao hu, qingmin jia, jinchao chen, yuan yao, yan pan, renchao xie, f.richard yu","distributed, parallel, and cluster computing (cs.dc)","multi-edge cooperative computing that combines constrained resources of multiple edges into a powerful resource pool has the potential to deliver great benefits, such as a tremendous computing power, improved response time, more diversified services. however, the mass heterogeneous resources composition and lack of scheduling strategies make the modeling and cooperating of multi-edge computing system particularly complicated. this paper first proposes a system-level state evaluation model to shield the complex hardware configurations and redefine the different service capabilities at heterogeneous edges. secondly, an integer linear programming model is designed to cater for optimally dispatching the distributed arriving requests. finally, a learning-based lightweight real-time scheduler, corais, is proposed. corais embeds the real-time states of multi-edge system and requests information, and combines the embeddings with a policy network to schedule the requests, so that the response time of all requests can be minimized. evaluation results verify that corais can make a high-quality scheduling decision in real time, and can be generalized to other multi-edge computing system, regardless of system scales. characteristic validation also demonstrates that corais successfully learns to balance loads, perceive real-time state and recognize heterogeneity while scheduling.",2024-02-04
"43",43,"leveraging constraint programming in a deep learning approach for dynamically solving the flexible job-shop scheduling problem","imanol echeverria, maialen murua, roberto santana","artificial intelligence (cs.ai)","recent advancements in the flexible job-shop scheduling problem (fjssp) are primarily based on deep reinforcement learning (drl) due to its ability to generate high-quality, real-time solutions. however, drl approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (cp), which can excel at finding optimal or near-optimal solutions for smaller instances. this paper aims to integrate cp within a deep learning (dl) based methodology, leveraging the benefits of both. in this paper, we introduce a method that involves training a dl model using optimal solutions generated by cp, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in drl and enhancing overall performance. further, we integrate cp into our dl framework to jointly construct solutions, utilizing dl for the initial complex stages and transitioning to cp for optimal resolution as the problem is simplified. our hybrid approach has been extensively tested on three public fjssp benchmarks, demonstrating superior performance over five state-of-the-art drl approaches and a widely-used cp solver. additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known drl method.",2024-03-14
"44",44,"quantum dynamic programming","jeongrak son, marek gluza, ryuji takagi, nelly h. y. ng","quantum physics (quant-ph)","we introduce a quantum extension of dynamic programming, a fundamental computational method for efficiently solving recursive problems using memory. our innovation lies in showing how to coherently generate unitaries of recursion steps using memorized intermediate quantum states. we find that quantum dynamic programming yields an exponential reduction in circuit depth for a large class of fixed-point quantum recursions, including a known recursive variant of the grover's search. additionally, we apply quantum dynamic programming to a recently proposed double-bracket quantum algorithm for diagonalization to obtain a new protocol for obliviously preparing a quantum state in its schmidt basis, providing a potential pathway for revealing entanglement structures of unknown quantum states.",2024-03-14
"45",45,"formalizing date arithmetic and statically detecting ambiguities for the law","raphaël monat, aymeric fromherz, denis merigoux","programming languages (cs.pl)","legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time. unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users. in this work, we aim to provide a solid foundation for date arithmetic working on days, months and years. we first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the f proof assistant. building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations. we finally integrate our approach in the catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the catala implementation of the french housing benefits, leading to the discovery of several date-related ambiguities.",2024-03-13
"46",46,"understanding and evaluating developer behaviour in programming tasks","martin schröer, rainer koschke","software engineering (cs.se)","to evaluate how developers perform differently in solving programming tasks, i.e., which actions and behaviours are more beneficial to them than others and if there are any specific strategies and behaviours that may indicate good versus poor understanding of the task and program given to them, we used the mimesis plug-in to record developers' interactions with the ide. in a series of three studies we investigated the specific behaviour of developers solving a specific programming task. we focused on which source code files they visited, how they related pieces of code and knowledge to others and when and how successful they performed code edits. to cope with the variety of behaviours due to interpersonal differences such as different level of knowledge, development style or problem solving stratiegies, we used an abstraction of the observed behaviour, which enables for a better comparison between different individual attributes such as skill, speed and used stratiegies and also facilitates later automatic evaluation of behaviours, i.e. by using a software to react to.",2024-03-13
"47",47,"user-centric beam selection and precoding design for coordinated multiple-satellite systems","vu nguyen ha, duy h. n. nguyen, juan c.-m. duncan, jorge l. gonzalez-rios, juan a. vasquez, geoffrey eappen, luis m. garces-socarras, rakesh palisetty, symeon chatzinotas, bjorn ottersten","signal processing (eess.sp)","this paper introduces a joint optimization framework for user-centric beam selection and linear precoding (lp) design in a coordinated multiple-satellite (comsat) system, employing a digital-fourier-transform-based (dft) beamforming (bf) technique. regarding serving users at their target sinrs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing lp for every satellite-to-user transmission. these technical objectives are first framed as a complex mixed-integer programming (mip) challenge. to tackle this, we reformulate it into a joint cluster association and lp design problem. then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution. additionally, a simpler duality approach for rapid beam selection and lp design is presented for comparison purposes. simulation results underscore the effectiveness of our proposed schemes across various settings.",2024-03-13
"48",48,"mastering text, code and math simultaneously via fusing highly specialized language models","ning ding, yulin chen, ganqu cui, xingtai lv, weilin zhao, ruobing xie, bowen zhou, zhiyuan liu, maosong sun","computation and language (cs.cl)","underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (llms) that strive to achieve high performance across all three domains simultaneously. achieving a very high level of proficiency for an llm within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. in this paper, we propose to fuse models that are already highly-specialized directly. the proposed fusing framework, ultrafuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. a token-level gating mechanism is introduced to blend the specialists' outputs. a two-stage training strategy accompanied by balanced sampling is designed to ensure stability. to effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, ultrachat , which includes text, code, and mathematical content. this dataset comprises approximately , instructions and covers a wide range of topics in each domain. experiments show that our model could simultaneously achieve mastery of the three crucial domains.",2024-03-13
"49",49,"bayesflo: bayesian fault localization of complex software systems","yi ji, simon mak, ryan lekivetz, joseph morgan","software engineering (cs.se)","software testing is essential for the reliable development of complex software systems. a key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. existing fault localization methods, however, are largely deterministic, and thus do not provide a principled approach for assessing probabilistic risk of potential root causes, or for integrating domain and/or structural knowledge from test engineers. to address this, we propose a novel bayesian fault localization framework called bayesflo, which leverages a flexible bayesian model on potential root cause combinations. a key feature of bayesflo is its integration of the principles of combination hierarchy and heredity, which capture the structured nature of failure-inducing combinations. a critical challenge, however, is the sheer number of potential root cause scenarios to consider, which renders the computation of posterior root cause probabilities infeasible even for small software systems. we thus develop new algorithms for efficient computation of such probabilities, leveraging recent tools from integer programming and graph representations. we then demonstrate the effectiveness of bayesflo over state-of-the-art fault localization methods, in a suite of numerical experiments and in two motivating case studies on the jmp xgboost interface.",2024-03-12
"50",50,"improving memory dependence prediction with static analysis","luke panayi, rohan gandhi, jim whittaker, vassilios chouliaras, martin berger, paul kelly","programming languages (cs.pl)","this paper explores the potential of communicating information gained by static analysis from compilers to out-of-order (ooo) machines, focusing on the memory dependence predictor (mdp). the mdp enables loads to issue without all in-flight store addresses being known, with minimal memory order violations. we use llvm to find loads with no dependencies and label them via their opcode. these labelled loads skip making lookups into the mdp, improving prediction accuracy by reducing false dependencies. we communicate this information in a minimally intrusive way, i.e.~without introducing additional hardware costs or instruction bandwidth, providing these improvements without any additional overhead in the cpu. we find that in select cases in spec, a significant number of load instructions can skip interacting with the mdp and lead to a performance gain. these results point to greater possibilities for static analysis as a source of near zero cost performance gains in future cpu designs.",2024-03-12
"51",51,"spline trajectory tracking and obstacle avoidance for mobile agents via convex optimization","akua dickson, christos g. cassandras, roberto tron","systems and control (eess.sy)","we propose an output feedback control-based motion planning technique for agents to enable them to converge to a specified polynomial trajectory while imposing a set of safety constraints on our controller to avoid collisions within the free configuration space (polygonal environment). to achieve this, we ) decompose our polygonal environment into different overlapping cells ) write out our polynomial trajectories as the output of a reference dynamical system with given initial conditions ) formulate convergence and safety constraints as linear matrix inequalities (lmis) on our controller using control lyapunov functions (clfs) and control barrier functions (cbfs) and ) solve a semi-definite programming (sdp) problem with convergence and safety constraints imposed to synthesize a controller for each convex cell. extensive simulations are included to test our motion planning method under different initial conditions and different reference trajectories. the synthesized controller is robust to changes in initial conditions and is always safe relative to the boundaries of the polygonal environment.",2024-03-25
"52",52,"bi-objective optimization in role mining","jason crampton, eduard eiben, gregory gutin, daniel karapetyan, diptapriyo majumdar","cryptography and security (cs.cr)","role mining is a technique used to derive a role-based authorization policy from an existing policy. given a set of users $u$, a set of permissions $p$ and a user-permission authorization relation $\mahtit{upa}\subseteq u\times p$, a role mining algorithm seeks to compute a set of roles $r$, a user-role authorization relation $\mathit{ua}\subseteq u\times r$ and a permission-role authorization relation $\mathit{pa}\subseteq r\times p$, such that the composition of $\mathit{ua}$ and $\mathit{pa}$ is close (in some appropriate sense) to $\mathit{upa}$.
in this paper, we first introduce the generalized noise role mining problem (gnrm) -- a generalization of the minnoise role mining problem -- which we believe has considerable practical relevance. extending work of fomin et al., we show that gnrm is fixed parameter tractable, with parameter $r + k$, where $r$ is the number of roles in the solution and $k$ is the number of discrepancies between $\mathit{upa}$ and the relation defined by the composition of $\mathit{ua}$ and $\mathit{pa}$. we further introduce a bi-objective optimization variant of gnrm, where we wish to minimize both $r$ and $k$ subject to upper bounds $r\le \bar{r}$ and $k\le \bar{k}$, where $\bar{r}$ and $\bar{k}$ are constants. we show that the pareto front of this bi-objective optimization problem (bo-gnrm) can be computed in fixed-parameter tractable time with parameter $\bar{r}+\bar{k}$.
we then report the results of our experimental work using the integer programming solver gurobi to solve instances of bo-gnrm. our key findings are that (a) we obtained strong support that gurobi's performance is fixed-parameter tractable, (b) our results suggest that our techniques may be useful for role mining in practice, based on our experiments in the context of three well-known real-world authorization policies.",2024-03-25
"53",53,"power-aware sparse reflect beamforming in active ris-aided interference channels","ruizhe long, hu zhou, ying-chang liang","information theory (cs.it)","active reconfigurable intelligent surface (ris) has attracted significant attention in wireless communications, due to its reflecting elements (res) capable of reflecting incident signals with not only phase shifts but also amplitude amplifications. in this paper, we are interested in active ris-aided interference channels in which $k$ user pairs share the same time and frequency resources with the aid of active ris. thanks to the promising amplitude amplification capability, activating a moderate number of res, rather than all of them, is sufficient for the active ris to mitigate cross-channel interferences. motivated by this, we propose a power-aware sparse reflect beamforming design for the active ris-aided interference channels, which allows the active ris to flexibly adjust the number of activated res for the sake of reducing hardware and power costs. specifically, we establish the power consumption model in which only those activated res consume the biasing and operation power that supports the amplitude amplification, yielding an $\ell_$-norm power consumption function. based on the proposed model, we investigate a sum-rate maximization problem and an active ris power minimization problem by carefully designing the sparse reflect beamforming vector. to solve these problems, we first replace the nonconvex $\ell_$-norm function with an iterative reweighted $\ell_$-norm function. then, fractional programming is used to solve the sum-rate maximization, while semidefinite programming together with the difference-of-convex algorithm (dca) is used to solve the active ris power minimization. numerical results show that the proposed sparse designs can notably increase the sum rate of user pairs and decrease the power consumption of active ris in interference channels.",2024-03-25
"54",54,"a monte carlo simulation of the broad band x-ray emission of the accreting millisecond x-ray pulsar maxi j-","yuan you (), shuang-nan zhang (), zhaosheng li (), mingyu ge () (() key laboratory of particle astrophysics, institute of high energy physics, chinese academy of sciences, beijing, china, () key laboratory of stars and interstellar medium, xiangtan university, xiangtan university, xiangtan, hunan, china)","high energy astrophysical phenomena (astro-ph.he)","maxi j- is an accreting millisecond x-ray pulsar (amxp) discovered in . according to the insight-hxmt data, the pulsations of this source extend all the way to over  kev, and its pulse profiles change from a single peak in low-energy range to double peaks in high-energy range. in this work, we simulate its energy spectra and pulse profiles with a compton scattering monte carlo program. the simulation results suggest that the low energy x-ray source on the neutron star surface should be pencil-beamed radiations from the magnetic poles, and there should be a boundary layer in a hollow cylinder shape between the accretion disc and the neutron star surface: the up-scattering of the polar radiations in the boundary layer leads to the double peak structure of the high-energy pulse profile. under this boundary layer geometry, we suggest that the rarity of amxps can be caused by the smearing of the boundary layer. to estimate the mass m and radius r of accretion-powered millisecond pulsars whose surface radiations are badly polluted by the accretion disk and boundary layer, the impact of compton scattering in the boundary layer on the radiation should be removed before employing the x-ray pulse profile modeling method.",2024-03-25
"55",55,"percentile optimization in wireless networks- part i: power control for max-min-rate to sum-rate maximization (and everything in between)","ahmad ali khan, raviraj adve","information theory (cs.it)","improving throughput for cell-edge users through coordinated resource allocation has been a long-standing driver of research in wireless cellular networks. while a variety of wireless resource management problems focus on sum utility, max-min utility and proportional fair utility, these formulations do not explicitly cater to cell-edge users and can, in fact, be disadvantageous to them. in this two-part paper series, we introduce a new class of optimization problems called percentile programs, which allow us to explicitly formulate problems that target lower-percentile throughput optimization for cell-edge users. part i focuses on the class of least-percentile throughput maximization through power control. this class subsumes the well-known max-min and max-sum-rate optimization problems as special cases. apart from these two extremes, we show that least-percentile rate programs are non-convex, non-smooth and strongly np-hard in general for multiuser interference networks, making optimization extremely challenging. we propose cyclic maximization algorithms that transform the original problems into equivalent block-concave forms, thereby enabling guaranteed convergence to stationary points. comparisons with state-of-the-art optimization algorithms such as successive convex approximation and sequential quadratic programming reveal that our proposed algorithms achieve superior performance while computing solutions orders of magnitude faster.",2024-03-25
"56",56,"safe reinforcement learning for constrained markov decision processes with stochastic stopping time","abhijit mazumdar, rafal wisniewski, manuela l. bujorianu","machine learning (cs.lg)","in this paper, we present an online reinforcement learning algorithm for constrained markov decision processes with a safety constraint. despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. to this end, we propose an algorithm based on linear programming that does not require a process model. we show that the learned policy is safe with high confidence. we also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. finally, we provide simulation results to show the efficacy of the proposed algorithm. further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.",2024-03-23
"57",57,"operational experience and r&d results using the google cloud for high energy physics in the atlas experiment","fernando barreiro megino, kaushik de, johannes elmsheuser, alexei klimentov, mario lassnig, miles euell, nikolai hartmann, tadashi maeno, verena martinez outschoorn, jay ajitbhai sandesara, dustin sell","high energy physics - experiment (hep-ex)","the atlas experiment at cern relies on a worldwide distributed computing grid infrastructure to support its physics program at the large hadron collider. atlas has integrated cloud computing resources to complement its grid infrastructure and conducted an r&d program on google cloud platform. these initiatives leverage key features of commercial cloud providers: lightweight configuration and operation, elasticity and availability of diverse infrastructure. this paper examines the seamless integration of cloud computing services as a conventional grid site within the atlas workflow management and data management systems, while also offering new setups for interactive, parallel analysis. it underscores pivotal results that enhance the on-site computing model and outlines several r&d projects that have benefited from large-scale, elastic resource provisioning models. furthermore, this study discusses the impact of cloud-enabled r\&d projects in three domains: accelerators and ai/ml, arm cpus and columnar data analysis techniques.",2024-03-23
"58",58,"energy efficient design of active star-ris-aided swipt systems","sajad faramarzi, hosein zarini, sepideh javadi, mohammad robat mili, rui zhang, george k. karagiannidis, naofal al-dhahir","information theory (cs.it)","in this paper, we consider the downlink transmission of a multi-antenna base station (bs) supported by an active simultaneously transmitting and reconfigurable intelligent surface (star-ris) to serve single-antenna users via simultaneous wireless information and power transfer (swipt). in this context, we formulate an energy efficiency maximisation problem that jointly optimises the gain, element selection and phase shift matrices of the active star-ris, the transmit beamforming of the bs and the power splitting ratio of the users. with respect to the highly coupled and non-convex form of this problem, an alternating optimisation solution approach is proposed, using tools from convex optimisation and reinforcement learning. specifically, semi-definite relaxation (sdr), difference of concave functions (dc), and fractional programming techniques are employed to transform the non-convex optimisation problem into a convex form for optimising the bs beamforming vector and the power splitting ratio of the swipt. then, by integrating meta-learning with the modified deep deterministic policy gradient (ddpg) and soft actor-critical (sac) methods, a combinatorial reinforcement learning network is developed to optimise the element selection, gain and phase shift matrices of the active star-ris. our simulations show the effectiveness of the proposed resource allocation scheme. furthermore, our proposed active star-ris-based swipt system outperforms its passive counterpart by % on average.",2024-03-23
"59",59,"codeshell technical report","rui xie, zhengran zeng, zhuohao yu, chang gao, shikun zhang, wei ye","software engineering (cs.se)","code large language models mark a pivotal breakthrough in artificial intelligence. they are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. in this technical report, we present codeshell-base, a seven billion-parameter foundation model with k context length, showcasing exceptional proficiency in code comprehension. by incorporating grouped-query attention and rotary positional embedding into gpt-, codeshell-base integrates the structural merits of starcoder and codellama and forms its unique architectural design. we then carefully built a comprehensive data pre-processing process, including similar data deduplication, perplexity-based data filtering, and model-based data filtering. through this process, we have curated  billion high-quality pre-training data from github. benefiting from the high-quality data, codeshell-base outperforms codellama in humaneval after training on just  billion tokens ( epochs). we have conducted extensive experiments across multiple language datasets, including python, java, and c++, and the results indicate that our model possesses robust foundational capabilities in code comprehension and generation.",2024-03-23
"60",60,"ac: algebraic computation checker for circuit constraints in zkps","hao chen, minyu chen, ruibang liu, guoqiang li","software engineering (cs.se)","zkp systems have surged attention and held a fundamental role in contemporary cryptography. zk-snark protocols dominate the zkp usage, often implemented through arithmetic circuit programming paradigm. however, underconstrained or overconstrained circuits may lead to bugs. underconstrained circuits refer to circuits that lack the necessary constraints, resulting in unexpected solutions in the circuit and causing the verifier to accept a bogus witness. overconstrained circuits refer to circuits that are constrained excessively, resulting in the circuit lacking necessary solutions and causing the verifier to accept no witness, rendering the circuit meaningless. this paper introduces a novel approach for pinpointing two distinct types of bugs in zkp circuits. the method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving polynomial equation systems over a finite field by algebraic computation. the classification of verification results is refined, greatly enhancing the expressive power of the system. we proposed a tool, ac, to represent the implementation of this method. experiments demonstrate that ac represents a substantial % increase in the checked ratio compared to prior work. within a solvable range, the checking time of ac has also exhibited noticeable improvement, demonstrating a magnitude increase compared to previous efforts.",2024-03-23
"61",61,"(towards a) statistical probabilistic lazy lambda calculus","radha jagadeesan","logic in computer science (cs.lo)","we study the desiderata on a model for statistical probabilistic programming languages. we argue that they can be met by a combination of traditional tools, namely open bisimulation and probabilistic simulation.",2024-03-22
"62",62,"programmers prefer individually assigned tasks vs. shared responsibility","adela krylova, roman makarov, sergei pasynkov, yegor bugayenko","software engineering (cs.se)","in traditional management, tasks are typically assigned to individuals, with each worker taking full responsibility for the success or failure of a task. in contrast, modern agile, lean, and extreme programming practices advocate for shared responsibility, where an entire group is accountable for the outcome of a project or task. despite numerous studies in other domains, the preferences of programmers have not been thoroughly analyzed. to address this gap, we conducted a survey featuring seven situational questions and collected the opinions of  software development practitioners. our findings reveal that programmers prefer tasks to be assigned to them on an individual basis and appreciate taking personal responsibility for failures, as well as receiving individual rewards for successes. understanding these preferences is crucial for project managers aiming to optimize team dynamics and ensure the successful completion of software projects.",2024-03-22
"63",63,"real-time safety index adaptation for parameter-varying systems via determinant gradient ascend","rui chen, weiye zhao, ruixuan liu, weiyang zhang, changliu liu","systems and control (eess.sy)","safety index synthesis (sis) is critical for deriving safe control laws. recent works propose to synthesize a safety index (si) via nonlinear programming and derive a safe control law such that the system ) achieves forward invariant (fi) with some safe set and ) guarantees finite time convergence (ftc) to that safe set. however, real-world system dynamics can vary during run-time, making the control law infeasible and invalidating the initial si. since the full sis nonlinear programming is computationally expensive, it is infeasible to re-synthesize the si each time the dynamics are perturbed. to address that, this paper proposes an efficient approach to adapting the si to varying system dynamics and maintaining the feasibility of the safe control law. the proposed method leverages determinant gradient ascend and derives a closed-form update to safety index parameters, enabling real-time adaptation performance. a numerical study validates the effectiveness of our approach.",2024-03-22
"64",64,"fat api bindings of c++ objects into scripting languages","russell k. standish","programming languages (cs.pl)","a fat api exposes nearly all of a c++ object's public attributes and methods to a consuming environment, such as a scripting language, or web client. this can be contrasted with a conventional, or thin api, where the api is defined up front, and the c++ object provides the implementation, most of which is private to the c++ layer. obviously, reflection is required to expose c++ objects to a consuming layer like this -- this paper explores using the classdesc system to implement reflection of c++ objects into a javascript/typescript environment via a restservice, and also via a node.js api module.",2024-03-22
"65",65,"a survey of neural code intelligence: paradigms, advances and beyond","qiushi sun, zhirui chen, fangzhi xu, kanzhi cheng, chang ma, zhangyue yin, jianing wang, chengcheng han, renyu zhu, shuai yuan, qipeng guo, xipeng qiu, pengcheng yin, xiaoli li, fei yuan, lingpeng kong, xiang li, zhiyong wu","software engineering (cs.se)","neural code intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. bridging the gap between natural language and programming language, this domain has drawn significant attention from researchers in both research communities over the past few years. this survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over  representative models and their variants, more than  categories of tasks, and an extensive coverage of over  related works. we follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of large language models). concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. for applications, we also observe a co-evolving shift. it spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. an ongoing, dynamically updated project and resources associated with this survey have been released at this https url.",2024-03-21
"66",66,"personalized programming guidance based on deep programming learning style capturing","yingfan liu, renyu zhu, ming gao","computers and society (cs.cy)","with the rapid development of big data and ai technology, programming is in high demand and has become an essential skill for students. meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. however, two significant challenges still need to be addressed in programming: c) how to recognize complex programming behaviors; c) how to capture intrinsic learning patterns that align with the actual learning process. to fill these gaps, in this paper, we propose a novel model called programming exercise recommender with learning style (pers), which simulates learners' intricate programming behaviors. specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to capture the changes of consecutive code submissions (which addresses c). to better profile programming behaviors, we extend the felder-silverman learning style model, a classical pedagogical theory, to perceive intrinsic programming patterns. based on this, we align three latent vectors to record and update programming ability, processing style, and understanding style, respectively (which addresses c). we perform extensive experiments on two real-world datasets to verify the rationality of modeling programming learning styles and the effectiveness of pers for personalized programming guidance.",2024-02-20
"67",67,"designing robust linear output feedback controller based on clf-cbf framework via linear~programming(lp-clf-cbf)","mahroo bahreinian, mehdi kermanshah, roberto tron","systems and control (eess.sy)","we consider the problem of designing output feedback controllers that use measurements from a set of landmarks to navigate through a cell-decomposable environment using duality, control lyapunov and barrier functions (clf, cbf), and linear programming. we propose two objectives for navigating in an environment, one to traverse the environment by making loops and one by converging to a stabilization point while smoothing the transition between consecutive cells. we test our algorithms in a simulation environment, evaluating the robustness of the approach to practical conditions, such as bearing-only measurements, and measurements acquired with a camera with a limited field of view.",2024-03-21
"68",68,"early planet formation in embedded disks (edisk) xiii: aligned disks with non-settled dust around the newly resolved class  protobinary r cra iras ","frankie j. encalada, leslie w. looney, shigehisa takakuwa, john j. tobin, nagayoshi ohashi, jes k. jørgensen, zhi-yun li, yuri aikawa, yusuke aso, patrick m. koch, woojin kwon, shih-ping lai, chang won lee, zhe-yu daniel lin, alejandro santamarıa-miranda, itziar de gregorio-monsalvo, nguyen thi phuong, adele plunkett, jinshi sai (insa choi), rajeeb sharma, hsi-wei yen, ilseung han","solar and stellar astrophysics (astro-ph.sr)","young protostellar binary systems, with expected ages less than $\sim$$^$ years, are little modified since birth, providing key clues to binary formation and evolution. we present a first look at the young, class  binary protostellar system r cra iras  from the early planet formation in embedded disks (edisk) alma large program, which observed the system in the . mm continuum emission, $^{}$co (-), $^{}$co (-), c$^{}$o (-), so ($_$-$_$), and nine other molecular lines that trace disk, envelope, shocks, and outflows. with a continuum resolution of $\sim$.$^{\prime\prime}$ ($\sim$ au, at a distance of  pc), we characterize the newly discovered binary system with a separation of  au, their circumstellar disks, and a circumbinary disk-like structure. the circumstellar disk radii are .$\pm$. and .$\pm$. au for sources a and b, respectively, and their circumstellar disk dust masses are estimated as .$\pm$. and .$\pm$. m$_{\earth}$. the circumstellar disks and the circumbinary structure have well aligned position angles and inclinations, indicating formation in a smooth, ordered process such as disk fragmentation. in addition, the circumstellar disks have a near/far-side asymmetry in the continuum emission suggesting that the dust has yet to settle into a thin layer near the midplane. spectral analysis of co isotopologues reveals outflows that originate from both of the sources and possibly from the circumbinary disk-like structure. furthermore, we detect keplerian rotation in the $^{}$co isotopologues toward both circumstellar disks and likely keplerian rotation in the circumbinary structure; the latter suggests that it is probably a circumbinary disk.",2024-03-21
"69",69,"can chatgpt detect deepfakes? a study of using multimodal large language models for media forensics","shan jia, reilin lyu, kangran zhao, yize chen, zhiyuan yan, yan ju, chuanbo hu, xin li, baoyuan wu, siwei lyu","artificial intelligence (cs.ai)","deepfakes, which refer to ai-generated media content, have become an increasing concern due to their use as a means for disinformation. detecting deepfakes is currently solved with programmed machine learning algorithms. in this work, we investigate the capabilities of multimodal large language models (llms) in deepfake detection. we conducted qualitative and quantitative experiments to demonstrate multimodal llms and show that they can expose ai-generated images through careful experimental design and prompt engineering. this is interesting, considering that llms are not inherently tailored for media forensic tasks, and the process does not require programming. we discuss the limitations of multimodal llms for these tasks and suggest possible improvements.",2024-03-21
"70",70,"real groups, symmetric varieties and langlands duality","tsao-hsien chen, david nadler","representation theory (math.rt)","let $g_\mathbb r$ be a connected real reductive group and let $x$ be the corresponding complex symmetric variety under the cartan bijection. we construct a canonical equivalence between the relative satake category of $g(\mathcal o)$-equivariant $\mathbb c$-constructible complexes on the loop space of $x$ and the real satake category of $g_\mathbb r(\mathcal o_\mathbb r)$-equivariant $\mathbb c$-constructible complexes on the real affine grassmannian. we show that the equivalence is $t$-exact with respect to the natural perverse $t$-structures and is compatible with the fusion products and hecke actions. we further show that the relative satake category is equivalent to the category of $\mathbb c$-constructible complexes on the moduli stack of $g_\mathbb r$-bundles on the real projective line $\mathbb p^(\mathbb r)$ and hence provides a connection between the relative langlands program and the geometric langlands program for real groups. we provide numerous applications of the main theorems to real and relative langlands duality including the formality and commutativity conjectures for the real and relative satake categories and an identification of the dual groups for $g_\mathbb r$ and $x$.",2024-03-20
"71",71,"depyf: open the opaque box of pytorch compiler for machine learning researchers","kaichao you, runsheng bai, meng cao, jianmin wang, ion stoica, mingsheng long","machine learning (cs.lg)","pytorch \texttt{.x} introduces a compiler designed to accelerate deep learning programs. however, for machine learning researchers, adapting to the pytorch compiler to full potential can be challenging. the compiler operates at the python bytecode level, making it appear as an opaque box. to address this, we introduce \texttt{depyf}, a tool designed to demystify the inner workings of the pytorch compiler. \texttt{depyf} decompiles bytecode generated by pytorch back into equivalent source code, and establishes connections between in-memory code objects and their on-disk source code counterparts. this feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. notably, \texttt{depyf} is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. the project is \href{this https url}{ openly available} and is recognized as a \href{this https url}{pytorch ecosystem project}.",2024-03-14
"72",72,"taming differentiable logics with coq formalisation","reynald affeldt, alessandro bruni, ekaterina komendantskaya, natalia ślusarz, kathrin stark","logic in computer science (cs.lo)","for performance and verification in machine learning, new methods have recently been proposed that optimise learning systems to satisfy formally expressed logical properties. among these methods, differentiable logics (dls) are used to translate propositional or first-order formulae into loss functions deployed for optimisation in machine learning. at the same time, recent attempts to give programming language support for verification of neural networks showed that dls can be used to compile verification properties to machine-learning backends. this situation is calling for stronger guarantees about the soundness of such compilers, the soundness and compositionality of dls, and the differentiability and performance of the resulting loss functions. in this paper, we propose an approach to formalise existing dls using the mathematical components library in the coq proof assistant. thanks to this formalisation, we are able to give uniform semantics to otherwise disparate dls, give formal proofs to existing informal arguments, find errors in previous work, and provide formal proofs to missing conjectured properties. this work is meant as a stepping stone for the development of programming language support for verification of machine learning.",2024-03-20
"73",73,"tensor quantum programming","a. termanova, ar. melnikov, e. mamenchikov, n. belokonev, s. dolgov, a. berezutskii, r. ellerbrock, c. mansell, m. perelshtein","quantum physics (quant-ph)","running quantum algorithms often involves implementing complex quantum circuits with such a large number of multi-qubit gates that the challenge of tackling practical applications appears daunting. to date, no experiments have successfully demonstrated a quantum advantage due to the ease with which the results can be adequately replicated on classical computers through the use of tensor network algorithms. additionally, it remains unclear even in theory where exactly these advantages are rooted within quantum systems because the logarithmic complexity commonly associated with quantum algorithms is also present in algorithms based on tensor networks. in this article, we propose a novel approach called tensor quantum programming, which leverages tensor networks for hybrid quantum computing. our key insight is that the primary challenge of algorithms based on tensor networks lies in their high ranks (bond dimensions). quantum computing offers a potential solution to this challenge, as an ideal quantum computer can represent tensors with arbitrarily high ranks in contrast to classical counterparts, which indicates the way towards quantum advantage. while tensor-based vector-encoding and state-readout are known procedures, the matrix-encoding required for performing matrix-vector multiplications directly on quantum devices remained unsolved. here, we developed an algorithm that encodes matrix product operators into quantum circuits with a depth that depends linearly on the number of qubits. it demonstrates effectiveness on up to  qubits for several matrices frequently encountered in differential equations, optimization problems, and quantum chemistry. we view this work as an initial stride towards the creation of genuinely practical quantum algorithms.",2024-03-20
"74",74,"regent based parallel meshfree lskum solver for heterogenous hpc platforms","sanath salil, nischay ram mamidi, anil nemili, elliott slaughter","distributed, parallel, and cluster computing (cs.dc)","regent is an implicitly parallel programming language that allows the development of a single codebase for heterogeneous platforms targeting cpus and gpus. this paper presents the development of a parallel meshfree solver in regent for two-dimensional inviscid compressible flows. the meshfree solver is based on the least squares kinetic upwind method. example codes are presented to show the difference between the regent and cuda-c implementations of the meshfree solver on a gpu node. for cpu parallel computations, details are presented on how the data communication and synchronisation are handled by regent and fortran+mpi codes. the regent solver is verified by applying it to the standard test cases for inviscid flows. benchmark simulations are performed on coarse to very fine point distributions to assess the solver's performance. the computational efficiency of the regent solver on an a gpu is compared with an equivalent meshfree solver written in cuda-c. the codes are then profiled to investigate the differences in their performance. the performance of the regent solver on cpu cores is compared with an equivalent explicitly parallel fortran meshfree solver based on mpi. scalability results are shown to offer insights into performance.",2024-03-20
"75",75,"c analyzer : a static program analysis tool for c programs","rajendra kumar solanki","programming languages (cs.pl)","in our times, when the world is increasingly becoming more dependent on software programs, writing bug-free, correct programs is crucial. program verification based on formal methods can guarantee this by detecting run-time errors in safety-critical systems to avoid possible adverse impacts on human life and save time and money.
this project work tries to leverage abstract interpretation techniques for static analysis of c programs. c analyzer is a tool developed for static analysis of c programs. this implementation of c analyzer provides a plug-and-play domain architecture for multiple abstract domains to be used. c analyzer supports four abstract domains - interval, octagon, polyhedra, and bit vector. we use these different domains for required precision in program verification. c analyzer tool uses llvm c/c++ compiler frontend clang api to generate and traverse the control flow graph (cfg) of a given c program. this tool generates invariants in different abstract domains for statements in basic blocks of cfg during cfg traversal. using these invariants, some properties of a program, such as dividing by zero, modulus zero, arithmetic overflow, etc., can be analyzed. we also use a source-to-source transformation tool, cil (common intermediate language), to transform some c constructs into simpler constructs, such as transforming logical operators, switch statements, and conditional operators into if-else ladders and transforming do-while and for loops into while loops.
using c analyzer, c program constructs such as declarations, assignments, binary operations (arithmetic, relational, bitwise shift, etc.), conditions (if-else), loops (while, do while, for loop), nested conditions, and nested loops can be analyzed. currently, this tool does not support arrays, structures, unions, pointers, or function calls.",2024-01-28
"76",76,"navigating compiler errors with ai assistance -- a study of gpt hints in an introductory programming course","maciej pankiewicz, ryan s. baker","software engineering (cs.se)","we examined the efficacy of ai-assisted learning in an introductory programming course at the university level by using a gpt- model to generate personalized hints for compiler errors within a platform for automated assessment of programming assignments. the control group had no access to gpt hints. in the experimental condition gpt hints were provided when a compiler error was detected, for the first half of the problems in each module. for the latter half of the module, hints were disabled. students highly rated the usefulness of gpt hints. in affect surveys, the experimental group reported significantly higher levels of focus and lower levels of confrustion (confusion and frustration) than the control group. for the six most commonly occurring error types we observed mixed results in terms of performance when access to gpt hints was enabled for the experimental group. however, in the absence of gpt hints, the experimental group's performance surpassed the control group for five out of the six error types.",2024-03-19
"77",77,"prototipo de video juego activo basado en una cámara d para motivar la actividad física en niños y adultos mayores","benjamín ojeda magaña, josé guadalupe robledo hernández, leopoldo gómez barba, victor manuel rangel cobián","human-computer interaction (cs.hc)","this document describes the development of a video game prototype designed to encourage physical activity among children and older adults. the prototype consists of a laptop, a camera with d sensors, and optionally requires an lcd screen or a projector. the programming component of this prototype was developed in scratch, a programming language geared towards children, which greatly facilitates the creation of a game tailored to the users' preferences. the idea to create such a prototype originated from the desire to offer an option that promotes physical activity among children and adults, given that a lack of physical exercise is a predominant factor in the development of chronic degenerative diseases such as diabetes and hypertension, to name the most common. as a result of this initiative, an active video game prototype was successfully developed, based on a ping-pong game, which allows both children and adults to interact in a fun way while encouraging the performance of physical activities that can positively impact the users' health.",2024-03-19
"78",78,"ikspark: an inverse kinematics solver using semidefinite relaxation and rank minimization","liangting wu, roberto tron","robotics (cs.ro)","inverse kinematics (ik) is a fundamental problem frequently occurred in robot control and motion planning. however, the problem is nonconvex because the kinematic map between the configuration and task spaces is generally nonlinear, which makes it challenging for fast and accurate solutions. the problem can be more complicated with the existence of different physical constraints imposed by the robot structure. in this paper, we develop an inverse kinematics solver named ikspark (inverse kinematics using semidefinite programming and rank minimization) that can find solutions for robots with various structures, including open/closed kinematic chains, spherical, revolute, and/or prismatic joints. the solver works in the space of rotation matrices of the link reference frames and involves solving only convex semidefinite problems (sdps). specifically, the ik problem is formulated as an sdp with an additional rank- constraint on symmetric matrices with constant traces. the solver first solves this sdp disregarding the rank constraint to get a start point and then finds the rank- solution iteratively via a rank minimization algorithm with proven local convergence. compared to other work that performs sdp relaxation for ik problems, our formulation is simpler, and uses variables with smaller sizes. we validate our approach via simulations on different robots, comparing against a standard ik method.",2024-03-18
"79",79,"routing and scheduling in answer set programming applied to multi-agent path finding: preliminary report","roland kaminski, torsten schaub, tran cao son, jiří švancara, philipp wanko","artificial intelligence (cs.ai)","we present alternative approaches to routing and scheduling in answer set programming (asp), and explore them in the context of multi-agent path finding. the idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. this also abolishes the need for fixed upper bounds on the length of plans. the trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. while this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in asp in a feasible way. this is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. we formally elaborate upon this idea and present several resulting asp encodings. finally, we demonstrate their effectiveness via an empirical analysis.",2024-03-18
"80",80,"semidefinite programming on population clustering: a local analysis","shuheng zhou","statistics theory (math.st)","in this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $$ sub-gaussian distributions. in particular, we design and analyze two computational efficient algorithms to partition data into two groups approximately according to their population of origin given a small sample in a recent paper (zhou a). our work is motivated by the application of clustering individuals according to their population of origin using markers, when the divergence between any two of the populations is small. moreover, we are interested in the case that individual features are of low average quality $\gamma$, and we want to use as few of them as possible to correctly partition the sample. here we use $p \gamma$ to denote the $\ell_^$ distance between two population centers (mean vectors), namely, $\mu^{()}$, $\mu^{()}$ $\in$ ${\mathbb r}^p$. we allow a full range of tradeoffs between $n, p, \gamma$ in the sense that partial recovery (success rate $< \%$) is feasible once the signal to noise ratio $s^ := \min\{np \gamma^, p \gamma\}$ is lower bounded by a constant. our work builds upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features in zhou (a). more importantly, we prove that the misclassification error decays exponentially with respect to the snr $s^$ in the present paper. the significance of such an exponentially decaying error bound is: when $s^ =\omega(\log n)$, perfect recovery of the cluster structure is accomplished. this result was introduced in zhou (a) without a proof. we therefore present the full proof in the present work.",2023-11-23
"81",81,"beamforming design for semantic-bit coexisting communication system","maojun zhang, guangxu zhu, richeng jin, xiaoming chen, qingjiang shi, caijun zhong, kaibin huang","information theory (cs.it)","semantic communication (semcom) is emerging as a key technology for future sixth-generation (g) systems. unlike traditional bit-level communication (bitcom), semcom directly optimizes performance at the semantic level, leading to superior communication efficiency. nevertheless, the task-oriented nature of semcom renders it challenging to completely replace bitcom. consequently, it is desired to consider a semantic-bit coexisting communication system, where a base station (bs) serves semcom users (sem-users) and bitcom users (bit-users) simultaneously. such a system faces severe and heterogeneous inter-user interference. in this context, this paper provides a new semantic-bit coexisting communication framework and proposes a spatial beamforming scheme to accommodate both types of users. specifically, we consider maximizing the semantic rate for semantic users while ensuring the quality-of-service (qos) requirements for bit-users. due to the intractability of obtaining the exact closed-form expression of the semantic rate, a data driven method is first applied to attain an approximated expression via data fitting. with the resulting complex transcendental function, majorization minimization (mm) is adopted to convert the original formulated problem into a multiple-ratio problem, which allows fractional programming (fp) to be used to further transform the problem into an inhomogeneous quadratically constrained quadratic programs (qcqp) problem. solving the problem leads to a semi-closed form solution with undetermined lagrangian factors that can be updated by a fixed point algorithm. extensive simulation results demonstrate that the proposed beamforming scheme significantly outperforms conventional beamforming algorithms such as zero-forcing (zf), maximum ratio transmission (mrt), and weighted minimum mean-square error (wmmse).",2024-03-18
"82",82,"looper: a learned automatic code optimizer for polyhedral compilers","massinissa merouani, khaled afif boudaoud, iheb nassim aouadj, nassim tchoulak, islem kara bernou, hamza benyamina, fatima benbouzid-si tayeb, karima benatchba, hugh leather, riyadh baghdadi","programming languages (cs.pl)","while polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. this has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. state-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. while such a proof-of-concept has shown promise, it still has significant limitations. state-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. they also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. these limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach. in this paper, we introduce looper, the first polyhedral autoscheduler that uses a deep-learning based cost model and covers a large set of affine transformations and programs. it supports the exploration of a large set of affine transformations, allowing the application of complex sequences of polyhedral transformations. it also supports the optimization of programs with multiple loop nests and with rectangular and non-rectangular iteration domains, allowing the optimization of an extensive set of programs. we implement and evaluate looper and show that it achieves speedups over the state-of-the-art. on the polybench benchmark, looper achieves a geometric mean speedup of .x over tiramisu. looper also achieves competitive speedups with a geometric mean speedup of .x over pluto, a state-of-the-art polyhedral compiler that does not use a machine-learning based cost model.",2024-03-18
"83",83,"table-lookup mac: scalable processing of quantised neural networks in fpga soft logic","daniel gerlinghoff, benjamin chen ming choong, rick siow mong goh, weng-fai wong, tao luo","hardware architecture (cs.ar)","recent advancements in neural network quantisation have yielded remarkable outcomes, with three-bit networks reaching state-of-the-art full-precision accuracy in complex tasks. these achievements present valuable opportunities for accelerating neural networks by computing in reduced precision. implementing it on fpgas can take advantage of bit-level reconfigurability, which is not available on conventional cpus and gpus. simultaneously, the high data intensity of neural network processing has inspired computing-in-memory paradigms, including on fpga platforms. by programming the effects of trained model weights as lookup operations in soft logic, the transfer of weight data from memory units can be avoided, alleviating the memory bottleneck. however, previous methods face poor scalability - the high logic utilisation limiting them to small networks/sub-networks of binary models with low accuracy. in this paper, we introduce table lookup multiply-accumulate (tlmac) as a framework to compile and optimise quantised neural networks for scalable lookup-based processing. tlmac clusters and maps unique groups of weights to lookup-based processing elements, enabling highly parallel computation while taking advantage of parameter redundancy. further place and route algorithms are proposed to reduce lut utilisation and routing congestion. we demonstrate that tlmac significantly improves the scalability of previous related works. our efficient logic mapping and high degree of reuse enables entire imagenet-scale quantised models with full-precision accuracy to be implemented using lookup-based computing on one commercially available fpga.",2024-03-18
"84",84,"multiscale quantile regression with local error control","zhi liu, housen li","methodology (stat.me)","for robust and efficient detection of change points, we introduce a novel methodology muscle (multiscale quantile segmentation controlling local error) that partitions serial data into multiple segments, each sharing a common quantile. it leverages multiple tests for quantile changes over different scales and locations, and variational estimation. unlike the often adopted global error control, muscle focuses on local errors defined on individual segments, significantly improving detection power in finding change points. meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter. further, we obtain the consistency and the localisation error rates in estimating change points, under mild signal-to-noise-ratio conditions. both match (up to log factors) the minimax optimality results in the gaussian setup. all theories hold under the only distributional assumption of serial independence. incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing muscle. extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness. an implementation via r package muscle is available from github.",2024-03-17
"85",85,"frequency-reactive power optimization strategy of grid-forming offshore wind farm using dru-hvdc transmission","zhekai li, kun han, xu cai, renxin yang, haotian yu, kepeng xia, lulu liu","optimization and control (math.oc)","the diode rectifier unit-based high voltage direct current (dru-hvdc) transmission with grid-forming (gfm) wind turbine is becoming a promising scheme for offshore wind farm(owf) integration due to its high reliability and low cost. in this scheme, the ac network of the owf and the dru has completely different synchronization mechanisms and power flow characteristics from the traditional power system. to optimize the power flow and reduce the net loss, this paper carries out the power flow modeling and optimization analysis for the dru-hvdc transmission system with grid-forming owfs. the influence of the dru and the gfm wind turbines on the power flow of the system is analyzed. on this basis, improved constraint conditions are proposed and an optimal power flow (opf) method is established. this method can minimize the power loss by adjusting the reactive power output of each wind turbine and internal network frequency. finally, based on matlab, this paper uses yalmip toolkit and cplex mathematical solver to realize the programming solution of the opf model proposed in this paper. the results show that the proposed optimization strategy can effectively reduce the power loss of the entire owf and the transmission system with an optimization ratio of network losses exceeding .%.",2024-03-16
"86",86,"mitigation and optimization of induced seismicity using physics-based forecasting","ryley g hill, matthew weingarten, cornelius langenbruch, yuri fialko","geophysics (physics.geo-ph)","fluid injection can induce seismicity by altering stresses on pre-existing faults. here, we investigate minimizing induced seismic hazard by optimizing injection operations in a physics-based forecasting framework. we built a d finite element model of the poroelastic crust for the raton basin, central us, and used it to estimate time dependent coulomb stress changes due to ~ years of wastewater injection in the region. our finite element model is complemented by a statistical analysis of the seismogenic index (si), a proxy for critically stressed faults affected by variations in the pore pressure. forecasts of seismicity rate from our hybrid physics-based statistical model suggest that induced seismicity in the raton basin, from  - , is still driven by wastewater injection. our model suggests that pore pressure diffusion is the dominant cause of coulomb stress changes at seismogenic depth, with poroelastic stress changes contributing about % to the driving force. linear programming optimization for the raton basin reveals that it is feasible to reduce seismic hazard for a given amount of injected fluid (safety objective) or maximize fluid injection for a prescribed seismic hazard (economic objective). the optimization tends to spread out high-rate injectors and shift them to regions of lower si. the framework has practical importance as a tool to manage injection rate per unit field area to reduce induced seismic hazard. our optimization framework is both flexible and adaptable to mitigate induced seismic hazard in other regions and for other types of subsurface fluid injection.",2024-03-15
"87",87,"unsupervised threat hunting using continuous bag-of-terms-and-time (cbott)","varol kayhan, shivendu shivendu, rouzbeh behnia, clinton daniel, manish agrawal","cryptography and security (cs.cr)","threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures. it can be performed in several ways, one of which is based on detecting anomalies. we propose an unsupervised framework, called continuous bag-of-terms-and-time (cbott), and publish its application programming interface (api) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among siem logs geared toward process auditing on endpoint devices. analyses show that our framework consistently outperforms benchmark approaches. when logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between .-.) while benchmark approaches identify the same anomalies at lower percentiles (between .-.). this framework can be used by other researchers to conduct benchmark analyses and cybersecurity analysts to find anomalies in siem logs.",2024-03-15
"88",88,"lyznet: a lightweight python tool for learning and verifying neural lyapunov functions and regions of attraction","jun liu, yiming meng, maxwell fitzsimmons, ruikun zhou","systems and control (eess.sy)","in this paper, we describe a lightweight python framework that provides integrated learning and verification of neural lyapunov functions for stability analysis. the proposed tool, named lyznet, learns neural lyapunov functions using physics-informed neural networks (pinns) to solve zubov's equation and verifies them using satisfiability modulo theories (smt) solvers. what distinguishes this tool from others in the literature is its ability to provide verified regions of attraction close to the domain of attraction. this is achieved by encoding zubov's partial differential equation (pde) into the pinn approach. by embracing the non-convex nature of the underlying optimization problems, we demonstrate that in cases where convex optimization, such as semidefinite programming, fails to capture the domain of attraction, our neural network framework proves more successful. the tool also offers automatic decomposition of coupled nonlinear systems into a network of low-dimensional subsystems for compositional verification. we illustrate the tool's usage and effectiveness with several numerical examples, including both non-trivial low-dimensional nonlinear systems and high-dimensional systems. the repository of the tool can be found at this https url.",2024-03-15
"89",89,"compositionally verifiable vector neural lyapunov functions for stability analysis of interconnected nonlinear systems","jun liu, yiming meng, maxwell fitzsimmons, ruikun zhou","systems and control (eess.sy)","while there has been increasing interest in using neural networks to compute lyapunov functions, verifying that these functions satisfy the lyapunov conditions and certifying stability regions remain challenging due to the curse of dimensionality. in this paper, we demonstrate that by leveraging the compositional structure of interconnected nonlinear systems, it is possible to verify neural lyapunov functions for high-dimensional systems beyond the capabilities of current satisfiability modulo theories (smt) solvers using a monolithic approach. our numerical examples employ neural lyapunov functions trained by solving zubov's partial differential equation (pde), which characterizes the domain of attraction for individual subsystems. these examples show a performance advantage over sums-of-squares (sos) polynomial lyapunov functions derived from semidefinite programming.",2024-03-15
"90",90,"one-shot learning for mips with sos constraints","charly robinson la rocca, jean-françois cordeau, emma frejinger","optimization and control (math.oc)","efficient algorithms and solvers are required to provide optimal or near-optimal solutions quickly and enable organizations to react promptly to dynamic situations such as supply chain disruptions or changing customer demands. state-of-the-art mixed-integer programming (mip) solvers are crafted to tackle a wide variety of problems, yet many real-world situations are characterized by problem instances that originate from a narrow distribution. this has inspired the creation of tailored approaches that exploit historical data to inform heuristic design. deep learning (dl) methods are typically used in this context to extract patterns from data, but they require large datasets and comprehensive hyperparameter tuning for strong performance. this article describes a one-shot learning heuristic that leverages solutions discovered within the branch-and-bound tree to construct a model with minimal overhead. we evaluate our method on the locomotive assignment problem (lap) and sets of miplib instances that contain constraints based on special ordered sets of type . experimental results include a comparison with multiple primal heuristics and state-of-the-art mip solvers. we show that the method is most effective with cplex in terms of the average primal gap.",2024-03-14
"91",91,"teaching machines to code: smart contract translation with llms","rabimba karanjai, lei xu, weidong shi","software engineering (cs.se)","the advent of large language models (llms) has marked a significant milestone in the realm of artificial intelligence, with their capabilities often matching or surpassing human expertise in various domains. among these achievements, their adeptness in translation tasks stands out, closely mimicking the intricate and preliminary processes undertaken by human translators to ensure the fidelity and quality of the translated content. despite the advancements in utilizing llms for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the llm, remains largely unexplored. in our research, we present a pioneering approach, solmover, which harnesses the synergy of two distinct llms within a unified framework. this framework is designed to grasp coding principles and apply this understanding to the translation of code into an unfamiliar language. our study delves into the capacity of llms to mimic human learning processes, offering an in-depth evaluation of our methodology for converting smart contracts written in solidity to move, a language with limited resources. the framework employs one llm to decipher coding conventions for the new language, creating a blueprint for the second llm, which, lacking planning abilities, possesses coding expertise. the empirical evidence from our experiments suggests that solmover substantially enhances performance compared to gpt-.-turbo-, and achieves superior results over competitors such as palm and mixtral-xb-instruct. additionally, our analysis highlights the efficacy of our bug mitigation strategy in elevating code quality across all models, even outside the solmover framework.",2024-03-13
"92",92,"corais: lightweight real-time scheduler for multi-edge cooperative computing","yujiao hu, qingmin jia, jinchao chen, yuan yao, yan pan, renchao xie, f.richard yu","distributed, parallel, and cluster computing (cs.dc)","multi-edge cooperative computing that combines constrained resources of multiple edges into a powerful resource pool has the potential to deliver great benefits, such as a tremendous computing power, improved response time, more diversified services. however, the mass heterogeneous resources composition and lack of scheduling strategies make the modeling and cooperating of multi-edge computing system particularly complicated. this paper first proposes a system-level state evaluation model to shield the complex hardware configurations and redefine the different service capabilities at heterogeneous edges. secondly, an integer linear programming model is designed to cater for optimally dispatching the distributed arriving requests. finally, a learning-based lightweight real-time scheduler, corais, is proposed. corais embeds the real-time states of multi-edge system and requests information, and combines the embeddings with a policy network to schedule the requests, so that the response time of all requests can be minimized. evaluation results verify that corais can make a high-quality scheduling decision in real time, and can be generalized to other multi-edge computing system, regardless of system scales. characteristic validation also demonstrates that corais successfully learns to balance loads, perceive real-time state and recognize heterogeneity while scheduling.",2024-02-04
"93",93,"leveraging constraint programming in a deep learning approach for dynamically solving the flexible job-shop scheduling problem","imanol echeverria, maialen murua, roberto santana","artificial intelligence (cs.ai)","recent advancements in the flexible job-shop scheduling problem (fjssp) are primarily based on deep reinforcement learning (drl) due to its ability to generate high-quality, real-time solutions. however, drl approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (cp), which can excel at finding optimal or near-optimal solutions for smaller instances. this paper aims to integrate cp within a deep learning (dl) based methodology, leveraging the benefits of both. in this paper, we introduce a method that involves training a dl model using optimal solutions generated by cp, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in drl and enhancing overall performance. further, we integrate cp into our dl framework to jointly construct solutions, utilizing dl for the initial complex stages and transitioning to cp for optimal resolution as the problem is simplified. our hybrid approach has been extensively tested on three public fjssp benchmarks, demonstrating superior performance over five state-of-the-art drl approaches and a widely-used cp solver. additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known drl method.",2024-03-14
"94",94,"quantum dynamic programming","jeongrak son, marek gluza, ryuji takagi, nelly h. y. ng","quantum physics (quant-ph)","we introduce a quantum extension of dynamic programming, a fundamental computational method for efficiently solving recursive problems using memory. our innovation lies in showing how to coherently generate unitaries of recursion steps using memorized intermediate quantum states. we find that quantum dynamic programming yields an exponential reduction in circuit depth for a large class of fixed-point quantum recursions, including a known recursive variant of the grover's search. additionally, we apply quantum dynamic programming to a recently proposed double-bracket quantum algorithm for diagonalization to obtain a new protocol for obliviously preparing a quantum state in its schmidt basis, providing a potential pathway for revealing entanglement structures of unknown quantum states.",2024-03-14
"95",95,"formalizing date arithmetic and statically detecting ambiguities for the law","raphaël monat, aymeric fromherz, denis merigoux","programming languages (cs.pl)","legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time. unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users. in this work, we aim to provide a solid foundation for date arithmetic working on days, months and years. we first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the f proof assistant. building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations. we finally integrate our approach in the catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the catala implementation of the french housing benefits, leading to the discovery of several date-related ambiguities.",2024-03-13
"96",96,"understanding and evaluating developer behaviour in programming tasks","martin schröer, rainer koschke","software engineering (cs.se)","to evaluate how developers perform differently in solving programming tasks, i.e., which actions and behaviours are more beneficial to them than others and if there are any specific strategies and behaviours that may indicate good versus poor understanding of the task and program given to them, we used the mimesis plug-in to record developers' interactions with the ide. in a series of three studies we investigated the specific behaviour of developers solving a specific programming task. we focused on which source code files they visited, how they related pieces of code and knowledge to others and when and how successful they performed code edits. to cope with the variety of behaviours due to interpersonal differences such as different level of knowledge, development style or problem solving stratiegies, we used an abstraction of the observed behaviour, which enables for a better comparison between different individual attributes such as skill, speed and used stratiegies and also facilitates later automatic evaluation of behaviours, i.e. by using a software to react to.",2024-03-13
"97",97,"user-centric beam selection and precoding design for coordinated multiple-satellite systems","vu nguyen ha, duy h. n. nguyen, juan c.-m. duncan, jorge l. gonzalez-rios, juan a. vasquez, geoffrey eappen, luis m. garces-socarras, rakesh palisetty, symeon chatzinotas, bjorn ottersten","signal processing (eess.sp)","this paper introduces a joint optimization framework for user-centric beam selection and linear precoding (lp) design in a coordinated multiple-satellite (comsat) system, employing a digital-fourier-transform-based (dft) beamforming (bf) technique. regarding serving users at their target sinrs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing lp for every satellite-to-user transmission. these technical objectives are first framed as a complex mixed-integer programming (mip) challenge. to tackle this, we reformulate it into a joint cluster association and lp design problem. then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution. additionally, a simpler duality approach for rapid beam selection and lp design is presented for comparison purposes. simulation results underscore the effectiveness of our proposed schemes across various settings.",2024-03-13
"98",98,"mastering text, code and math simultaneously via fusing highly specialized language models","ning ding, yulin chen, ganqu cui, xingtai lv, weilin zhao, ruobing xie, bowen zhou, zhiyuan liu, maosong sun","computation and language (cs.cl)","underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (llms) that strive to achieve high performance across all three domains simultaneously. achieving a very high level of proficiency for an llm within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. in this paper, we propose to fuse models that are already highly-specialized directly. the proposed fusing framework, ultrafuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. a token-level gating mechanism is introduced to blend the specialists' outputs. a two-stage training strategy accompanied by balanced sampling is designed to ensure stability. to effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, ultrachat , which includes text, code, and mathematical content. this dataset comprises approximately , instructions and covers a wide range of topics in each domain. experiments show that our model could simultaneously achieve mastery of the three crucial domains.",2024-03-13
"99",99,"bayesflo: bayesian fault localization of complex software systems","yi ji, simon mak, ryan lekivetz, joseph morgan","software engineering (cs.se)","software testing is essential for the reliable development of complex software systems. a key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. existing fault localization methods, however, are largely deterministic, and thus do not provide a principled approach for assessing probabilistic risk of potential root causes, or for integrating domain and/or structural knowledge from test engineers. to address this, we propose a novel bayesian fault localization framework called bayesflo, which leverages a flexible bayesian model on potential root cause combinations. a key feature of bayesflo is its integration of the principles of combination hierarchy and heredity, which capture the structured nature of failure-inducing combinations. a critical challenge, however, is the sheer number of potential root cause scenarios to consider, which renders the computation of posterior root cause probabilities infeasible even for small software systems. we thus develop new algorithms for efficient computation of such probabilities, leveraging recent tools from integer programming and graph representations. we then demonstrate the effectiveness of bayesflo over state-of-the-art fault localization methods, in a suite of numerical experiments and in two motivating case studies on the jmp xgboost interface.",2024-03-12
"100",100,"improving memory dependence prediction with static analysis","luke panayi, rohan gandhi, jim whittaker, vassilios chouliaras, martin berger, paul kelly","programming languages (cs.pl)","this paper explores the potential of communicating information gained by static analysis from compilers to out-of-order (ooo) machines, focusing on the memory dependence predictor (mdp). the mdp enables loads to issue without all in-flight store addresses being known, with minimal memory order violations. we use llvm to find loads with no dependencies and label them via their opcode. these labelled loads skip making lookups into the mdp, improving prediction accuracy by reducing false dependencies. we communicate this information in a minimally intrusive way, i.e.~without introducing additional hardware costs or instruction bandwidth, providing these improvements without any additional overhead in the cpu. we find that in select cases in spec, a significant number of load instructions can skip interacting with the mdp and lead to a performance gain. these results point to greater possibilities for static analysis as a source of near zero cost performance gains in future cpu designs.",2024-03-12
"101",101,"spline trajectory tracking and obstacle avoidance for mobile agents via convex optimization","akua dickson, christos g. cassandras, roberto tron","systems and control (eess.sy)","we propose an output feedback control-based motion planning technique for agents to enable them to converge to a specified polynomial trajectory while imposing a set of safety constraints on our controller to avoid collisions within the free configuration space (polygonal environment). to achieve this, we ) decompose our polygonal environment into different overlapping cells ) write out our polynomial trajectories as the output of a reference dynamical system with given initial conditions ) formulate convergence and safety constraints as linear matrix inequalities (lmis) on our controller using control lyapunov functions (clfs) and control barrier functions (cbfs) and ) solve a semi-definite programming (sdp) problem with convergence and safety constraints imposed to synthesize a controller for each convex cell. extensive simulations are included to test our motion planning method under different initial conditions and different reference trajectories. the synthesized controller is robust to changes in initial conditions and is always safe relative to the boundaries of the polygonal environment.",2024-03-25
"102",102,"bi-objective optimization in role mining","jason crampton, eduard eiben, gregory gutin, daniel karapetyan, diptapriyo majumdar","cryptography and security (cs.cr)","role mining is a technique used to derive a role-based authorization policy from an existing policy. given a set of users $u$, a set of permissions $p$ and a user-permission authorization relation $\mahtit{upa}\subseteq u\times p$, a role mining algorithm seeks to compute a set of roles $r$, a user-role authorization relation $\mathit{ua}\subseteq u\times r$ and a permission-role authorization relation $\mathit{pa}\subseteq r\times p$, such that the composition of $\mathit{ua}$ and $\mathit{pa}$ is close (in some appropriate sense) to $\mathit{upa}$.
in this paper, we first introduce the generalized noise role mining problem (gnrm) -- a generalization of the minnoise role mining problem -- which we believe has considerable practical relevance. extending work of fomin et al., we show that gnrm is fixed parameter tractable, with parameter $r + k$, where $r$ is the number of roles in the solution and $k$ is the number of discrepancies between $\mathit{upa}$ and the relation defined by the composition of $\mathit{ua}$ and $\mathit{pa}$. we further introduce a bi-objective optimization variant of gnrm, where we wish to minimize both $r$ and $k$ subject to upper bounds $r\le \bar{r}$ and $k\le \bar{k}$, where $\bar{r}$ and $\bar{k}$ are constants. we show that the pareto front of this bi-objective optimization problem (bo-gnrm) can be computed in fixed-parameter tractable time with parameter $\bar{r}+\bar{k}$.
we then report the results of our experimental work using the integer programming solver gurobi to solve instances of bo-gnrm. our key findings are that (a) we obtained strong support that gurobi's performance is fixed-parameter tractable, (b) our results suggest that our techniques may be useful for role mining in practice, based on our experiments in the context of three well-known real-world authorization policies.",2024-03-25
"103",103,"power-aware sparse reflect beamforming in active ris-aided interference channels","ruizhe long, hu zhou, ying-chang liang","information theory (cs.it)","active reconfigurable intelligent surface (ris) has attracted significant attention in wireless communications, due to its reflecting elements (res) capable of reflecting incident signals with not only phase shifts but also amplitude amplifications. in this paper, we are interested in active ris-aided interference channels in which $k$ user pairs share the same time and frequency resources with the aid of active ris. thanks to the promising amplitude amplification capability, activating a moderate number of res, rather than all of them, is sufficient for the active ris to mitigate cross-channel interferences. motivated by this, we propose a power-aware sparse reflect beamforming design for the active ris-aided interference channels, which allows the active ris to flexibly adjust the number of activated res for the sake of reducing hardware and power costs. specifically, we establish the power consumption model in which only those activated res consume the biasing and operation power that supports the amplitude amplification, yielding an $\ell_$-norm power consumption function. based on the proposed model, we investigate a sum-rate maximization problem and an active ris power minimization problem by carefully designing the sparse reflect beamforming vector. to solve these problems, we first replace the nonconvex $\ell_$-norm function with an iterative reweighted $\ell_$-norm function. then, fractional programming is used to solve the sum-rate maximization, while semidefinite programming together with the difference-of-convex algorithm (dca) is used to solve the active ris power minimization. numerical results show that the proposed sparse designs can notably increase the sum rate of user pairs and decrease the power consumption of active ris in interference channels.",2024-03-25
"104",104,"a monte carlo simulation of the broad band x-ray emission of the accreting millisecond x-ray pulsar maxi j-","yuan you (), shuang-nan zhang (), zhaosheng li (), mingyu ge () (() key laboratory of particle astrophysics, institute of high energy physics, chinese academy of sciences, beijing, china, () key laboratory of stars and interstellar medium, xiangtan university, xiangtan university, xiangtan, hunan, china)","high energy astrophysical phenomena (astro-ph.he)","maxi j- is an accreting millisecond x-ray pulsar (amxp) discovered in . according to the insight-hxmt data, the pulsations of this source extend all the way to over  kev, and its pulse profiles change from a single peak in low-energy range to double peaks in high-energy range. in this work, we simulate its energy spectra and pulse profiles with a compton scattering monte carlo program. the simulation results suggest that the low energy x-ray source on the neutron star surface should be pencil-beamed radiations from the magnetic poles, and there should be a boundary layer in a hollow cylinder shape between the accretion disc and the neutron star surface: the up-scattering of the polar radiations in the boundary layer leads to the double peak structure of the high-energy pulse profile. under this boundary layer geometry, we suggest that the rarity of amxps can be caused by the smearing of the boundary layer. to estimate the mass m and radius r of accretion-powered millisecond pulsars whose surface radiations are badly polluted by the accretion disk and boundary layer, the impact of compton scattering in the boundary layer on the radiation should be removed before employing the x-ray pulse profile modeling method.",2024-03-25
"105",105,"percentile optimization in wireless networks- part i: power control for max-min-rate to sum-rate maximization (and everything in between)","ahmad ali khan, raviraj adve","information theory (cs.it)","improving throughput for cell-edge users through coordinated resource allocation has been a long-standing driver of research in wireless cellular networks. while a variety of wireless resource management problems focus on sum utility, max-min utility and proportional fair utility, these formulations do not explicitly cater to cell-edge users and can, in fact, be disadvantageous to them. in this two-part paper series, we introduce a new class of optimization problems called percentile programs, which allow us to explicitly formulate problems that target lower-percentile throughput optimization for cell-edge users. part i focuses on the class of least-percentile throughput maximization through power control. this class subsumes the well-known max-min and max-sum-rate optimization problems as special cases. apart from these two extremes, we show that least-percentile rate programs are non-convex, non-smooth and strongly np-hard in general for multiuser interference networks, making optimization extremely challenging. we propose cyclic maximization algorithms that transform the original problems into equivalent block-concave forms, thereby enabling guaranteed convergence to stationary points. comparisons with state-of-the-art optimization algorithms such as successive convex approximation and sequential quadratic programming reveal that our proposed algorithms achieve superior performance while computing solutions orders of magnitude faster.",2024-03-25
"106",106,"safe reinforcement learning for constrained markov decision processes with stochastic stopping time","abhijit mazumdar, rafal wisniewski, manuela l. bujorianu","machine learning (cs.lg)","in this paper, we present an online reinforcement learning algorithm for constrained markov decision processes with a safety constraint. despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. to this end, we propose an algorithm based on linear programming that does not require a process model. we show that the learned policy is safe with high confidence. we also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. finally, we provide simulation results to show the efficacy of the proposed algorithm. further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.",2024-03-23
"107",107,"operational experience and r&d results using the google cloud for high energy physics in the atlas experiment","fernando barreiro megino, kaushik de, johannes elmsheuser, alexei klimentov, mario lassnig, miles euell, nikolai hartmann, tadashi maeno, verena martinez outschoorn, jay ajitbhai sandesara, dustin sell","high energy physics - experiment (hep-ex)","the atlas experiment at cern relies on a worldwide distributed computing grid infrastructure to support its physics program at the large hadron collider. atlas has integrated cloud computing resources to complement its grid infrastructure and conducted an r&d program on google cloud platform. these initiatives leverage key features of commercial cloud providers: lightweight configuration and operation, elasticity and availability of diverse infrastructure. this paper examines the seamless integration of cloud computing services as a conventional grid site within the atlas workflow management and data management systems, while also offering new setups for interactive, parallel analysis. it underscores pivotal results that enhance the on-site computing model and outlines several r&d projects that have benefited from large-scale, elastic resource provisioning models. furthermore, this study discusses the impact of cloud-enabled r\&d projects in three domains: accelerators and ai/ml, arm cpus and columnar data analysis techniques.",2024-03-23
"108",108,"energy efficient design of active star-ris-aided swipt systems","sajad faramarzi, hosein zarini, sepideh javadi, mohammad robat mili, rui zhang, george k. karagiannidis, naofal al-dhahir","information theory (cs.it)","in this paper, we consider the downlink transmission of a multi-antenna base station (bs) supported by an active simultaneously transmitting and reconfigurable intelligent surface (star-ris) to serve single-antenna users via simultaneous wireless information and power transfer (swipt). in this context, we formulate an energy efficiency maximisation problem that jointly optimises the gain, element selection and phase shift matrices of the active star-ris, the transmit beamforming of the bs and the power splitting ratio of the users. with respect to the highly coupled and non-convex form of this problem, an alternating optimisation solution approach is proposed, using tools from convex optimisation and reinforcement learning. specifically, semi-definite relaxation (sdr), difference of concave functions (dc), and fractional programming techniques are employed to transform the non-convex optimisation problem into a convex form for optimising the bs beamforming vector and the power splitting ratio of the swipt. then, by integrating meta-learning with the modified deep deterministic policy gradient (ddpg) and soft actor-critical (sac) methods, a combinatorial reinforcement learning network is developed to optimise the element selection, gain and phase shift matrices of the active star-ris. our simulations show the effectiveness of the proposed resource allocation scheme. furthermore, our proposed active star-ris-based swipt system outperforms its passive counterpart by % on average.",2024-03-23
"109",109,"codeshell technical report","rui xie, zhengran zeng, zhuohao yu, chang gao, shikun zhang, wei ye","software engineering (cs.se)","code large language models mark a pivotal breakthrough in artificial intelligence. they are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. in this technical report, we present codeshell-base, a seven billion-parameter foundation model with k context length, showcasing exceptional proficiency in code comprehension. by incorporating grouped-query attention and rotary positional embedding into gpt-, codeshell-base integrates the structural merits of starcoder and codellama and forms its unique architectural design. we then carefully built a comprehensive data pre-processing process, including similar data deduplication, perplexity-based data filtering, and model-based data filtering. through this process, we have curated  billion high-quality pre-training data from github. benefiting from the high-quality data, codeshell-base outperforms codellama in humaneval after training on just  billion tokens ( epochs). we have conducted extensive experiments across multiple language datasets, including python, java, and c++, and the results indicate that our model possesses robust foundational capabilities in code comprehension and generation.",2024-03-23
"110",110,"ac: algebraic computation checker for circuit constraints in zkps","hao chen, minyu chen, ruibang liu, guoqiang li","software engineering (cs.se)","zkp systems have surged attention and held a fundamental role in contemporary cryptography. zk-snark protocols dominate the zkp usage, often implemented through arithmetic circuit programming paradigm. however, underconstrained or overconstrained circuits may lead to bugs. underconstrained circuits refer to circuits that lack the necessary constraints, resulting in unexpected solutions in the circuit and causing the verifier to accept a bogus witness. overconstrained circuits refer to circuits that are constrained excessively, resulting in the circuit lacking necessary solutions and causing the verifier to accept no witness, rendering the circuit meaningless. this paper introduces a novel approach for pinpointing two distinct types of bugs in zkp circuits. the method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving polynomial equation systems over a finite field by algebraic computation. the classification of verification results is refined, greatly enhancing the expressive power of the system. we proposed a tool, ac, to represent the implementation of this method. experiments demonstrate that ac represents a substantial % increase in the checked ratio compared to prior work. within a solvable range, the checking time of ac has also exhibited noticeable improvement, demonstrating a magnitude increase compared to previous efforts.",2024-03-23
"111",111,"(towards a) statistical probabilistic lazy lambda calculus","radha jagadeesan","logic in computer science (cs.lo)","we study the desiderata on a model for statistical probabilistic programming languages. we argue that they can be met by a combination of traditional tools, namely open bisimulation and probabilistic simulation.",2024-03-22
"112",112,"programmers prefer individually assigned tasks vs. shared responsibility","adela krylova, roman makarov, sergei pasynkov, yegor bugayenko","software engineering (cs.se)","in traditional management, tasks are typically assigned to individuals, with each worker taking full responsibility for the success or failure of a task. in contrast, modern agile, lean, and extreme programming practices advocate for shared responsibility, where an entire group is accountable for the outcome of a project or task. despite numerous studies in other domains, the preferences of programmers have not been thoroughly analyzed. to address this gap, we conducted a survey featuring seven situational questions and collected the opinions of  software development practitioners. our findings reveal that programmers prefer tasks to be assigned to them on an individual basis and appreciate taking personal responsibility for failures, as well as receiving individual rewards for successes. understanding these preferences is crucial for project managers aiming to optimize team dynamics and ensure the successful completion of software projects.",2024-03-22
"113",113,"real-time safety index adaptation for parameter-varying systems via determinant gradient ascend","rui chen, weiye zhao, ruixuan liu, weiyang zhang, changliu liu","systems and control (eess.sy)","safety index synthesis (sis) is critical for deriving safe control laws. recent works propose to synthesize a safety index (si) via nonlinear programming and derive a safe control law such that the system ) achieves forward invariant (fi) with some safe set and ) guarantees finite time convergence (ftc) to that safe set. however, real-world system dynamics can vary during run-time, making the control law infeasible and invalidating the initial si. since the full sis nonlinear programming is computationally expensive, it is infeasible to re-synthesize the si each time the dynamics are perturbed. to address that, this paper proposes an efficient approach to adapting the si to varying system dynamics and maintaining the feasibility of the safe control law. the proposed method leverages determinant gradient ascend and derives a closed-form update to safety index parameters, enabling real-time adaptation performance. a numerical study validates the effectiveness of our approach.",2024-03-22
"114",114,"fat api bindings of c++ objects into scripting languages","russell k. standish","programming languages (cs.pl)","a fat api exposes nearly all of a c++ object's public attributes and methods to a consuming environment, such as a scripting language, or web client. this can be contrasted with a conventional, or thin api, where the api is defined up front, and the c++ object provides the implementation, most of which is private to the c++ layer. obviously, reflection is required to expose c++ objects to a consuming layer like this -- this paper explores using the classdesc system to implement reflection of c++ objects into a javascript/typescript environment via a restservice, and also via a node.js api module.",2024-03-22
"115",115,"a survey of neural code intelligence: paradigms, advances and beyond","qiushi sun, zhirui chen, fangzhi xu, kanzhi cheng, chang ma, zhangyue yin, jianing wang, chengcheng han, renyu zhu, shuai yuan, qipeng guo, xipeng qiu, pengcheng yin, xiaoli li, fei yuan, lingpeng kong, xiang li, zhiyong wu","software engineering (cs.se)","neural code intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. bridging the gap between natural language and programming language, this domain has drawn significant attention from researchers in both research communities over the past few years. this survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over  representative models and their variants, more than  categories of tasks, and an extensive coverage of over  related works. we follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of large language models). concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. for applications, we also observe a co-evolving shift. it spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. an ongoing, dynamically updated project and resources associated with this survey have been released at this https url.",2024-03-21
"116",116,"personalized programming guidance based on deep programming learning style capturing","yingfan liu, renyu zhu, ming gao","computers and society (cs.cy)","with the rapid development of big data and ai technology, programming is in high demand and has become an essential skill for students. meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. however, two significant challenges still need to be addressed in programming: c) how to recognize complex programming behaviors; c) how to capture intrinsic learning patterns that align with the actual learning process. to fill these gaps, in this paper, we propose a novel model called programming exercise recommender with learning style (pers), which simulates learners' intricate programming behaviors. specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to capture the changes of consecutive code submissions (which addresses c). to better profile programming behaviors, we extend the felder-silverman learning style model, a classical pedagogical theory, to perceive intrinsic programming patterns. based on this, we align three latent vectors to record and update programming ability, processing style, and understanding style, respectively (which addresses c). we perform extensive experiments on two real-world datasets to verify the rationality of modeling programming learning styles and the effectiveness of pers for personalized programming guidance.",2024-02-20
"117",117,"designing robust linear output feedback controller based on clf-cbf framework via linear~programming(lp-clf-cbf)","mahroo bahreinian, mehdi kermanshah, roberto tron","systems and control (eess.sy)","we consider the problem of designing output feedback controllers that use measurements from a set of landmarks to navigate through a cell-decomposable environment using duality, control lyapunov and barrier functions (clf, cbf), and linear programming. we propose two objectives for navigating in an environment, one to traverse the environment by making loops and one by converging to a stabilization point while smoothing the transition between consecutive cells. we test our algorithms in a simulation environment, evaluating the robustness of the approach to practical conditions, such as bearing-only measurements, and measurements acquired with a camera with a limited field of view.",2024-03-21
"118",118,"early planet formation in embedded disks (edisk) xiii: aligned disks with non-settled dust around the newly resolved class  protobinary r cra iras ","frankie j. encalada, leslie w. looney, shigehisa takakuwa, john j. tobin, nagayoshi ohashi, jes k. jørgensen, zhi-yun li, yuri aikawa, yusuke aso, patrick m. koch, woojin kwon, shih-ping lai, chang won lee, zhe-yu daniel lin, alejandro santamarıa-miranda, itziar de gregorio-monsalvo, nguyen thi phuong, adele plunkett, jinshi sai (insa choi), rajeeb sharma, hsi-wei yen, ilseung han","solar and stellar astrophysics (astro-ph.sr)","young protostellar binary systems, with expected ages less than $\sim$$^$ years, are little modified since birth, providing key clues to binary formation and evolution. we present a first look at the young, class  binary protostellar system r cra iras  from the early planet formation in embedded disks (edisk) alma large program, which observed the system in the . mm continuum emission, $^{}$co (-), $^{}$co (-), c$^{}$o (-), so ($_$-$_$), and nine other molecular lines that trace disk, envelope, shocks, and outflows. with a continuum resolution of $\sim$.$^{\prime\prime}$ ($\sim$ au, at a distance of  pc), we characterize the newly discovered binary system with a separation of  au, their circumstellar disks, and a circumbinary disk-like structure. the circumstellar disk radii are .$\pm$. and .$\pm$. au for sources a and b, respectively, and their circumstellar disk dust masses are estimated as .$\pm$. and .$\pm$. m$_{\earth}$. the circumstellar disks and the circumbinary structure have well aligned position angles and inclinations, indicating formation in a smooth, ordered process such as disk fragmentation. in addition, the circumstellar disks have a near/far-side asymmetry in the continuum emission suggesting that the dust has yet to settle into a thin layer near the midplane. spectral analysis of co isotopologues reveals outflows that originate from both of the sources and possibly from the circumbinary disk-like structure. furthermore, we detect keplerian rotation in the $^{}$co isotopologues toward both circumstellar disks and likely keplerian rotation in the circumbinary structure; the latter suggests that it is probably a circumbinary disk.",2024-03-21
"119",119,"can chatgpt detect deepfakes? a study of using multimodal large language models for media forensics","shan jia, reilin lyu, kangran zhao, yize chen, zhiyuan yan, yan ju, chuanbo hu, xin li, baoyuan wu, siwei lyu","artificial intelligence (cs.ai)","deepfakes, which refer to ai-generated media content, have become an increasing concern due to their use as a means for disinformation. detecting deepfakes is currently solved with programmed machine learning algorithms. in this work, we investigate the capabilities of multimodal large language models (llms) in deepfake detection. we conducted qualitative and quantitative experiments to demonstrate multimodal llms and show that they can expose ai-generated images through careful experimental design and prompt engineering. this is interesting, considering that llms are not inherently tailored for media forensic tasks, and the process does not require programming. we discuss the limitations of multimodal llms for these tasks and suggest possible improvements.",2024-03-21
"120",120,"real groups, symmetric varieties and langlands duality","tsao-hsien chen, david nadler","representation theory (math.rt)","let $g_\mathbb r$ be a connected real reductive group and let $x$ be the corresponding complex symmetric variety under the cartan bijection. we construct a canonical equivalence between the relative satake category of $g(\mathcal o)$-equivariant $\mathbb c$-constructible complexes on the loop space of $x$ and the real satake category of $g_\mathbb r(\mathcal o_\mathbb r)$-equivariant $\mathbb c$-constructible complexes on the real affine grassmannian. we show that the equivalence is $t$-exact with respect to the natural perverse $t$-structures and is compatible with the fusion products and hecke actions. we further show that the relative satake category is equivalent to the category of $\mathbb c$-constructible complexes on the moduli stack of $g_\mathbb r$-bundles on the real projective line $\mathbb p^(\mathbb r)$ and hence provides a connection between the relative langlands program and the geometric langlands program for real groups. we provide numerous applications of the main theorems to real and relative langlands duality including the formality and commutativity conjectures for the real and relative satake categories and an identification of the dual groups for $g_\mathbb r$ and $x$.",2024-03-20
"121",121,"depyf: open the opaque box of pytorch compiler for machine learning researchers","kaichao you, runsheng bai, meng cao, jianmin wang, ion stoica, mingsheng long","machine learning (cs.lg)","pytorch \texttt{.x} introduces a compiler designed to accelerate deep learning programs. however, for machine learning researchers, adapting to the pytorch compiler to full potential can be challenging. the compiler operates at the python bytecode level, making it appear as an opaque box. to address this, we introduce \texttt{depyf}, a tool designed to demystify the inner workings of the pytorch compiler. \texttt{depyf} decompiles bytecode generated by pytorch back into equivalent source code, and establishes connections between in-memory code objects and their on-disk source code counterparts. this feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. notably, \texttt{depyf} is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. the project is \href{this https url}{ openly available} and is recognized as a \href{this https url}{pytorch ecosystem project}.",2024-03-14
"122",122,"taming differentiable logics with coq formalisation","reynald affeldt, alessandro bruni, ekaterina komendantskaya, natalia ślusarz, kathrin stark","logic in computer science (cs.lo)","for performance and verification in machine learning, new methods have recently been proposed that optimise learning systems to satisfy formally expressed logical properties. among these methods, differentiable logics (dls) are used to translate propositional or first-order formulae into loss functions deployed for optimisation in machine learning. at the same time, recent attempts to give programming language support for verification of neural networks showed that dls can be used to compile verification properties to machine-learning backends. this situation is calling for stronger guarantees about the soundness of such compilers, the soundness and compositionality of dls, and the differentiability and performance of the resulting loss functions. in this paper, we propose an approach to formalise existing dls using the mathematical components library in the coq proof assistant. thanks to this formalisation, we are able to give uniform semantics to otherwise disparate dls, give formal proofs to existing informal arguments, find errors in previous work, and provide formal proofs to missing conjectured properties. this work is meant as a stepping stone for the development of programming language support for verification of machine learning.",2024-03-20
"123",123,"tensor quantum programming","a. termanova, ar. melnikov, e. mamenchikov, n. belokonev, s. dolgov, a. berezutskii, r. ellerbrock, c. mansell, m. perelshtein","quantum physics (quant-ph)","running quantum algorithms often involves implementing complex quantum circuits with such a large number of multi-qubit gates that the challenge of tackling practical applications appears daunting. to date, no experiments have successfully demonstrated a quantum advantage due to the ease with which the results can be adequately replicated on classical computers through the use of tensor network algorithms. additionally, it remains unclear even in theory where exactly these advantages are rooted within quantum systems because the logarithmic complexity commonly associated with quantum algorithms is also present in algorithms based on tensor networks. in this article, we propose a novel approach called tensor quantum programming, which leverages tensor networks for hybrid quantum computing. our key insight is that the primary challenge of algorithms based on tensor networks lies in their high ranks (bond dimensions). quantum computing offers a potential solution to this challenge, as an ideal quantum computer can represent tensors with arbitrarily high ranks in contrast to classical counterparts, which indicates the way towards quantum advantage. while tensor-based vector-encoding and state-readout are known procedures, the matrix-encoding required for performing matrix-vector multiplications directly on quantum devices remained unsolved. here, we developed an algorithm that encodes matrix product operators into quantum circuits with a depth that depends linearly on the number of qubits. it demonstrates effectiveness on up to  qubits for several matrices frequently encountered in differential equations, optimization problems, and quantum chemistry. we view this work as an initial stride towards the creation of genuinely practical quantum algorithms.",2024-03-20
"124",124,"regent based parallel meshfree lskum solver for heterogenous hpc platforms","sanath salil, nischay ram mamidi, anil nemili, elliott slaughter","distributed, parallel, and cluster computing (cs.dc)","regent is an implicitly parallel programming language that allows the development of a single codebase for heterogeneous platforms targeting cpus and gpus. this paper presents the development of a parallel meshfree solver in regent for two-dimensional inviscid compressible flows. the meshfree solver is based on the least squares kinetic upwind method. example codes are presented to show the difference between the regent and cuda-c implementations of the meshfree solver on a gpu node. for cpu parallel computations, details are presented on how the data communication and synchronisation are handled by regent and fortran+mpi codes. the regent solver is verified by applying it to the standard test cases for inviscid flows. benchmark simulations are performed on coarse to very fine point distributions to assess the solver's performance. the computational efficiency of the regent solver on an a gpu is compared with an equivalent meshfree solver written in cuda-c. the codes are then profiled to investigate the differences in their performance. the performance of the regent solver on cpu cores is compared with an equivalent explicitly parallel fortran meshfree solver based on mpi. scalability results are shown to offer insights into performance.",2024-03-20
"125",125,"c analyzer : a static program analysis tool for c programs","rajendra kumar solanki","programming languages (cs.pl)","in our times, when the world is increasingly becoming more dependent on software programs, writing bug-free, correct programs is crucial. program verification based on formal methods can guarantee this by detecting run-time errors in safety-critical systems to avoid possible adverse impacts on human life and save time and money.
this project work tries to leverage abstract interpretation techniques for static analysis of c programs. c analyzer is a tool developed for static analysis of c programs. this implementation of c analyzer provides a plug-and-play domain architecture for multiple abstract domains to be used. c analyzer supports four abstract domains - interval, octagon, polyhedra, and bit vector. we use these different domains for required precision in program verification. c analyzer tool uses llvm c/c++ compiler frontend clang api to generate and traverse the control flow graph (cfg) of a given c program. this tool generates invariants in different abstract domains for statements in basic blocks of cfg during cfg traversal. using these invariants, some properties of a program, such as dividing by zero, modulus zero, arithmetic overflow, etc., can be analyzed. we also use a source-to-source transformation tool, cil (common intermediate language), to transform some c constructs into simpler constructs, such as transforming logical operators, switch statements, and conditional operators into if-else ladders and transforming do-while and for loops into while loops.
using c analyzer, c program constructs such as declarations, assignments, binary operations (arithmetic, relational, bitwise shift, etc.), conditions (if-else), loops (while, do while, for loop), nested conditions, and nested loops can be analyzed. currently, this tool does not support arrays, structures, unions, pointers, or function calls.",2024-01-28
"126",126,"navigating compiler errors with ai assistance -- a study of gpt hints in an introductory programming course","maciej pankiewicz, ryan s. baker","software engineering (cs.se)","we examined the efficacy of ai-assisted learning in an introductory programming course at the university level by using a gpt- model to generate personalized hints for compiler errors within a platform for automated assessment of programming assignments. the control group had no access to gpt hints. in the experimental condition gpt hints were provided when a compiler error was detected, for the first half of the problems in each module. for the latter half of the module, hints were disabled. students highly rated the usefulness of gpt hints. in affect surveys, the experimental group reported significantly higher levels of focus and lower levels of confrustion (confusion and frustration) than the control group. for the six most commonly occurring error types we observed mixed results in terms of performance when access to gpt hints was enabled for the experimental group. however, in the absence of gpt hints, the experimental group's performance surpassed the control group for five out of the six error types.",2024-03-19
"127",127,"prototipo de video juego activo basado en una cámara d para motivar la actividad física en niños y adultos mayores","benjamín ojeda magaña, josé guadalupe robledo hernández, leopoldo gómez barba, victor manuel rangel cobián","human-computer interaction (cs.hc)","this document describes the development of a video game prototype designed to encourage physical activity among children and older adults. the prototype consists of a laptop, a camera with d sensors, and optionally requires an lcd screen or a projector. the programming component of this prototype was developed in scratch, a programming language geared towards children, which greatly facilitates the creation of a game tailored to the users' preferences. the idea to create such a prototype originated from the desire to offer an option that promotes physical activity among children and adults, given that a lack of physical exercise is a predominant factor in the development of chronic degenerative diseases such as diabetes and hypertension, to name the most common. as a result of this initiative, an active video game prototype was successfully developed, based on a ping-pong game, which allows both children and adults to interact in a fun way while encouraging the performance of physical activities that can positively impact the users' health.",2024-03-19
"128",128,"ikspark: an inverse kinematics solver using semidefinite relaxation and rank minimization","liangting wu, roberto tron","robotics (cs.ro)","inverse kinematics (ik) is a fundamental problem frequently occurred in robot control and motion planning. however, the problem is nonconvex because the kinematic map between the configuration and task spaces is generally nonlinear, which makes it challenging for fast and accurate solutions. the problem can be more complicated with the existence of different physical constraints imposed by the robot structure. in this paper, we develop an inverse kinematics solver named ikspark (inverse kinematics using semidefinite programming and rank minimization) that can find solutions for robots with various structures, including open/closed kinematic chains, spherical, revolute, and/or prismatic joints. the solver works in the space of rotation matrices of the link reference frames and involves solving only convex semidefinite problems (sdps). specifically, the ik problem is formulated as an sdp with an additional rank- constraint on symmetric matrices with constant traces. the solver first solves this sdp disregarding the rank constraint to get a start point and then finds the rank- solution iteratively via a rank minimization algorithm with proven local convergence. compared to other work that performs sdp relaxation for ik problems, our formulation is simpler, and uses variables with smaller sizes. we validate our approach via simulations on different robots, comparing against a standard ik method.",2024-03-18
"129",129,"routing and scheduling in answer set programming applied to multi-agent path finding: preliminary report","roland kaminski, torsten schaub, tran cao son, jiří švancara, philipp wanko","artificial intelligence (cs.ai)","we present alternative approaches to routing and scheduling in answer set programming (asp), and explore them in the context of multi-agent path finding. the idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. this also abolishes the need for fixed upper bounds on the length of plans. the trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. while this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in asp in a feasible way. this is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. we formally elaborate upon this idea and present several resulting asp encodings. finally, we demonstrate their effectiveness via an empirical analysis.",2024-03-18
"130",130,"semidefinite programming on population clustering: a local analysis","shuheng zhou","statistics theory (math.st)","in this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $$ sub-gaussian distributions. in particular, we design and analyze two computational efficient algorithms to partition data into two groups approximately according to their population of origin given a small sample in a recent paper (zhou a). our work is motivated by the application of clustering individuals according to their population of origin using markers, when the divergence between any two of the populations is small. moreover, we are interested in the case that individual features are of low average quality $\gamma$, and we want to use as few of them as possible to correctly partition the sample. here we use $p \gamma$ to denote the $\ell_^$ distance between two population centers (mean vectors), namely, $\mu^{()}$, $\mu^{()}$ $\in$ ${\mathbb r}^p$. we allow a full range of tradeoffs between $n, p, \gamma$ in the sense that partial recovery (success rate $< \%$) is feasible once the signal to noise ratio $s^ := \min\{np \gamma^, p \gamma\}$ is lower bounded by a constant. our work builds upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features in zhou (a). more importantly, we prove that the misclassification error decays exponentially with respect to the snr $s^$ in the present paper. the significance of such an exponentially decaying error bound is: when $s^ =\omega(\log n)$, perfect recovery of the cluster structure is accomplished. this result was introduced in zhou (a) without a proof. we therefore present the full proof in the present work.",2023-11-23
"131",131,"beamforming design for semantic-bit coexisting communication system","maojun zhang, guangxu zhu, richeng jin, xiaoming chen, qingjiang shi, caijun zhong, kaibin huang","information theory (cs.it)","semantic communication (semcom) is emerging as a key technology for future sixth-generation (g) systems. unlike traditional bit-level communication (bitcom), semcom directly optimizes performance at the semantic level, leading to superior communication efficiency. nevertheless, the task-oriented nature of semcom renders it challenging to completely replace bitcom. consequently, it is desired to consider a semantic-bit coexisting communication system, where a base station (bs) serves semcom users (sem-users) and bitcom users (bit-users) simultaneously. such a system faces severe and heterogeneous inter-user interference. in this context, this paper provides a new semantic-bit coexisting communication framework and proposes a spatial beamforming scheme to accommodate both types of users. specifically, we consider maximizing the semantic rate for semantic users while ensuring the quality-of-service (qos) requirements for bit-users. due to the intractability of obtaining the exact closed-form expression of the semantic rate, a data driven method is first applied to attain an approximated expression via data fitting. with the resulting complex transcendental function, majorization minimization (mm) is adopted to convert the original formulated problem into a multiple-ratio problem, which allows fractional programming (fp) to be used to further transform the problem into an inhomogeneous quadratically constrained quadratic programs (qcqp) problem. solving the problem leads to a semi-closed form solution with undetermined lagrangian factors that can be updated by a fixed point algorithm. extensive simulation results demonstrate that the proposed beamforming scheme significantly outperforms conventional beamforming algorithms such as zero-forcing (zf), maximum ratio transmission (mrt), and weighted minimum mean-square error (wmmse).",2024-03-18
"132",132,"looper: a learned automatic code optimizer for polyhedral compilers","massinissa merouani, khaled afif boudaoud, iheb nassim aouadj, nassim tchoulak, islem kara bernou, hamza benyamina, fatima benbouzid-si tayeb, karima benatchba, hugh leather, riyadh baghdadi","programming languages (cs.pl)","while polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. this has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. state-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. while such a proof-of-concept has shown promise, it still has significant limitations. state-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. they also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. these limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach. in this paper, we introduce looper, the first polyhedral autoscheduler that uses a deep-learning based cost model and covers a large set of affine transformations and programs. it supports the exploration of a large set of affine transformations, allowing the application of complex sequences of polyhedral transformations. it also supports the optimization of programs with multiple loop nests and with rectangular and non-rectangular iteration domains, allowing the optimization of an extensive set of programs. we implement and evaluate looper and show that it achieves speedups over the state-of-the-art. on the polybench benchmark, looper achieves a geometric mean speedup of .x over tiramisu. looper also achieves competitive speedups with a geometric mean speedup of .x over pluto, a state-of-the-art polyhedral compiler that does not use a machine-learning based cost model.",2024-03-18
"133",133,"table-lookup mac: scalable processing of quantised neural networks in fpga soft logic","daniel gerlinghoff, benjamin chen ming choong, rick siow mong goh, weng-fai wong, tao luo","hardware architecture (cs.ar)","recent advancements in neural network quantisation have yielded remarkable outcomes, with three-bit networks reaching state-of-the-art full-precision accuracy in complex tasks. these achievements present valuable opportunities for accelerating neural networks by computing in reduced precision. implementing it on fpgas can take advantage of bit-level reconfigurability, which is not available on conventional cpus and gpus. simultaneously, the high data intensity of neural network processing has inspired computing-in-memory paradigms, including on fpga platforms. by programming the effects of trained model weights as lookup operations in soft logic, the transfer of weight data from memory units can be avoided, alleviating the memory bottleneck. however, previous methods face poor scalability - the high logic utilisation limiting them to small networks/sub-networks of binary models with low accuracy. in this paper, we introduce table lookup multiply-accumulate (tlmac) as a framework to compile and optimise quantised neural networks for scalable lookup-based processing. tlmac clusters and maps unique groups of weights to lookup-based processing elements, enabling highly parallel computation while taking advantage of parameter redundancy. further place and route algorithms are proposed to reduce lut utilisation and routing congestion. we demonstrate that tlmac significantly improves the scalability of previous related works. our efficient logic mapping and high degree of reuse enables entire imagenet-scale quantised models with full-precision accuracy to be implemented using lookup-based computing on one commercially available fpga.",2024-03-18
"134",134,"multiscale quantile regression with local error control","zhi liu, housen li","methodology (stat.me)","for robust and efficient detection of change points, we introduce a novel methodology muscle (multiscale quantile segmentation controlling local error) that partitions serial data into multiple segments, each sharing a common quantile. it leverages multiple tests for quantile changes over different scales and locations, and variational estimation. unlike the often adopted global error control, muscle focuses on local errors defined on individual segments, significantly improving detection power in finding change points. meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter. further, we obtain the consistency and the localisation error rates in estimating change points, under mild signal-to-noise-ratio conditions. both match (up to log factors) the minimax optimality results in the gaussian setup. all theories hold under the only distributional assumption of serial independence. incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing muscle. extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness. an implementation via r package muscle is available from github.",2024-03-17
"135",135,"frequency-reactive power optimization strategy of grid-forming offshore wind farm using dru-hvdc transmission","zhekai li, kun han, xu cai, renxin yang, haotian yu, kepeng xia, lulu liu","optimization and control (math.oc)","the diode rectifier unit-based high voltage direct current (dru-hvdc) transmission with grid-forming (gfm) wind turbine is becoming a promising scheme for offshore wind farm(owf) integration due to its high reliability and low cost. in this scheme, the ac network of the owf and the dru has completely different synchronization mechanisms and power flow characteristics from the traditional power system. to optimize the power flow and reduce the net loss, this paper carries out the power flow modeling and optimization analysis for the dru-hvdc transmission system with grid-forming owfs. the influence of the dru and the gfm wind turbines on the power flow of the system is analyzed. on this basis, improved constraint conditions are proposed and an optimal power flow (opf) method is established. this method can minimize the power loss by adjusting the reactive power output of each wind turbine and internal network frequency. finally, based on matlab, this paper uses yalmip toolkit and cplex mathematical solver to realize the programming solution of the opf model proposed in this paper. the results show that the proposed optimization strategy can effectively reduce the power loss of the entire owf and the transmission system with an optimization ratio of network losses exceeding .%.",2024-03-16
"136",136,"mitigation and optimization of induced seismicity using physics-based forecasting","ryley g hill, matthew weingarten, cornelius langenbruch, yuri fialko","geophysics (physics.geo-ph)","fluid injection can induce seismicity by altering stresses on pre-existing faults. here, we investigate minimizing induced seismic hazard by optimizing injection operations in a physics-based forecasting framework. we built a d finite element model of the poroelastic crust for the raton basin, central us, and used it to estimate time dependent coulomb stress changes due to ~ years of wastewater injection in the region. our finite element model is complemented by a statistical analysis of the seismogenic index (si), a proxy for critically stressed faults affected by variations in the pore pressure. forecasts of seismicity rate from our hybrid physics-based statistical model suggest that induced seismicity in the raton basin, from  - , is still driven by wastewater injection. our model suggests that pore pressure diffusion is the dominant cause of coulomb stress changes at seismogenic depth, with poroelastic stress changes contributing about % to the driving force. linear programming optimization for the raton basin reveals that it is feasible to reduce seismic hazard for a given amount of injected fluid (safety objective) or maximize fluid injection for a prescribed seismic hazard (economic objective). the optimization tends to spread out high-rate injectors and shift them to regions of lower si. the framework has practical importance as a tool to manage injection rate per unit field area to reduce induced seismic hazard. our optimization framework is both flexible and adaptable to mitigate induced seismic hazard in other regions and for other types of subsurface fluid injection.",2024-03-15
"137",137,"unsupervised threat hunting using continuous bag-of-terms-and-time (cbott)","varol kayhan, shivendu shivendu, rouzbeh behnia, clinton daniel, manish agrawal","cryptography and security (cs.cr)","threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures. it can be performed in several ways, one of which is based on detecting anomalies. we propose an unsupervised framework, called continuous bag-of-terms-and-time (cbott), and publish its application programming interface (api) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among siem logs geared toward process auditing on endpoint devices. analyses show that our framework consistently outperforms benchmark approaches. when logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between .-.) while benchmark approaches identify the same anomalies at lower percentiles (between .-.). this framework can be used by other researchers to conduct benchmark analyses and cybersecurity analysts to find anomalies in siem logs.",2024-03-15
"138",138,"lyznet: a lightweight python tool for learning and verifying neural lyapunov functions and regions of attraction","jun liu, yiming meng, maxwell fitzsimmons, ruikun zhou","systems and control (eess.sy)","in this paper, we describe a lightweight python framework that provides integrated learning and verification of neural lyapunov functions for stability analysis. the proposed tool, named lyznet, learns neural lyapunov functions using physics-informed neural networks (pinns) to solve zubov's equation and verifies them using satisfiability modulo theories (smt) solvers. what distinguishes this tool from others in the literature is its ability to provide verified regions of attraction close to the domain of attraction. this is achieved by encoding zubov's partial differential equation (pde) into the pinn approach. by embracing the non-convex nature of the underlying optimization problems, we demonstrate that in cases where convex optimization, such as semidefinite programming, fails to capture the domain of attraction, our neural network framework proves more successful. the tool also offers automatic decomposition of coupled nonlinear systems into a network of low-dimensional subsystems for compositional verification. we illustrate the tool's usage and effectiveness with several numerical examples, including both non-trivial low-dimensional nonlinear systems and high-dimensional systems. the repository of the tool can be found at this https url.",2024-03-15
"139",139,"compositionally verifiable vector neural lyapunov functions for stability analysis of interconnected nonlinear systems","jun liu, yiming meng, maxwell fitzsimmons, ruikun zhou","systems and control (eess.sy)","while there has been increasing interest in using neural networks to compute lyapunov functions, verifying that these functions satisfy the lyapunov conditions and certifying stability regions remain challenging due to the curse of dimensionality. in this paper, we demonstrate that by leveraging the compositional structure of interconnected nonlinear systems, it is possible to verify neural lyapunov functions for high-dimensional systems beyond the capabilities of current satisfiability modulo theories (smt) solvers using a monolithic approach. our numerical examples employ neural lyapunov functions trained by solving zubov's partial differential equation (pde), which characterizes the domain of attraction for individual subsystems. these examples show a performance advantage over sums-of-squares (sos) polynomial lyapunov functions derived from semidefinite programming.",2024-03-15
"140",140,"one-shot learning for mips with sos constraints","charly robinson la rocca, jean-françois cordeau, emma frejinger","optimization and control (math.oc)","efficient algorithms and solvers are required to provide optimal or near-optimal solutions quickly and enable organizations to react promptly to dynamic situations such as supply chain disruptions or changing customer demands. state-of-the-art mixed-integer programming (mip) solvers are crafted to tackle a wide variety of problems, yet many real-world situations are characterized by problem instances that originate from a narrow distribution. this has inspired the creation of tailored approaches that exploit historical data to inform heuristic design. deep learning (dl) methods are typically used in this context to extract patterns from data, but they require large datasets and comprehensive hyperparameter tuning for strong performance. this article describes a one-shot learning heuristic that leverages solutions discovered within the branch-and-bound tree to construct a model with minimal overhead. we evaluate our method on the locomotive assignment problem (lap) and sets of miplib instances that contain constraints based on special ordered sets of type . experimental results include a comparison with multiple primal heuristics and state-of-the-art mip solvers. we show that the method is most effective with cplex in terms of the average primal gap.",2024-03-14
"141",141,"teaching machines to code: smart contract translation with llms","rabimba karanjai, lei xu, weidong shi","software engineering (cs.se)","the advent of large language models (llms) has marked a significant milestone in the realm of artificial intelligence, with their capabilities often matching or surpassing human expertise in various domains. among these achievements, their adeptness in translation tasks stands out, closely mimicking the intricate and preliminary processes undertaken by human translators to ensure the fidelity and quality of the translated content. despite the advancements in utilizing llms for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the llm, remains largely unexplored. in our research, we present a pioneering approach, solmover, which harnesses the synergy of two distinct llms within a unified framework. this framework is designed to grasp coding principles and apply this understanding to the translation of code into an unfamiliar language. our study delves into the capacity of llms to mimic human learning processes, offering an in-depth evaluation of our methodology for converting smart contracts written in solidity to move, a language with limited resources. the framework employs one llm to decipher coding conventions for the new language, creating a blueprint for the second llm, which, lacking planning abilities, possesses coding expertise. the empirical evidence from our experiments suggests that solmover substantially enhances performance compared to gpt-.-turbo-, and achieves superior results over competitors such as palm and mixtral-xb-instruct. additionally, our analysis highlights the efficacy of our bug mitigation strategy in elevating code quality across all models, even outside the solmover framework.",2024-03-13
"142",142,"corais: lightweight real-time scheduler for multi-edge cooperative computing","yujiao hu, qingmin jia, jinchao chen, yuan yao, yan pan, renchao xie, f.richard yu","distributed, parallel, and cluster computing (cs.dc)","multi-edge cooperative computing that combines constrained resources of multiple edges into a powerful resource pool has the potential to deliver great benefits, such as a tremendous computing power, improved response time, more diversified services. however, the mass heterogeneous resources composition and lack of scheduling strategies make the modeling and cooperating of multi-edge computing system particularly complicated. this paper first proposes a system-level state evaluation model to shield the complex hardware configurations and redefine the different service capabilities at heterogeneous edges. secondly, an integer linear programming model is designed to cater for optimally dispatching the distributed arriving requests. finally, a learning-based lightweight real-time scheduler, corais, is proposed. corais embeds the real-time states of multi-edge system and requests information, and combines the embeddings with a policy network to schedule the requests, so that the response time of all requests can be minimized. evaluation results verify that corais can make a high-quality scheduling decision in real time, and can be generalized to other multi-edge computing system, regardless of system scales. characteristic validation also demonstrates that corais successfully learns to balance loads, perceive real-time state and recognize heterogeneity while scheduling.",2024-02-04
"143",143,"leveraging constraint programming in a deep learning approach for dynamically solving the flexible job-shop scheduling problem","imanol echeverria, maialen murua, roberto santana","artificial intelligence (cs.ai)","recent advancements in the flexible job-shop scheduling problem (fjssp) are primarily based on deep reinforcement learning (drl) due to its ability to generate high-quality, real-time solutions. however, drl approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (cp), which can excel at finding optimal or near-optimal solutions for smaller instances. this paper aims to integrate cp within a deep learning (dl) based methodology, leveraging the benefits of both. in this paper, we introduce a method that involves training a dl model using optimal solutions generated by cp, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in drl and enhancing overall performance. further, we integrate cp into our dl framework to jointly construct solutions, utilizing dl for the initial complex stages and transitioning to cp for optimal resolution as the problem is simplified. our hybrid approach has been extensively tested on three public fjssp benchmarks, demonstrating superior performance over five state-of-the-art drl approaches and a widely-used cp solver. additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known drl method.",2024-03-14
"144",144,"quantum dynamic programming","jeongrak son, marek gluza, ryuji takagi, nelly h. y. ng","quantum physics (quant-ph)","we introduce a quantum extension of dynamic programming, a fundamental computational method for efficiently solving recursive problems using memory. our innovation lies in showing how to coherently generate unitaries of recursion steps using memorized intermediate quantum states. we find that quantum dynamic programming yields an exponential reduction in circuit depth for a large class of fixed-point quantum recursions, including a known recursive variant of the grover's search. additionally, we apply quantum dynamic programming to a recently proposed double-bracket quantum algorithm for diagonalization to obtain a new protocol for obliviously preparing a quantum state in its schmidt basis, providing a potential pathway for revealing entanglement structures of unknown quantum states.",2024-03-14
"145",145,"formalizing date arithmetic and statically detecting ambiguities for the law","raphaël monat, aymeric fromherz, denis merigoux","programming languages (cs.pl)","legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time. unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users. in this work, we aim to provide a solid foundation for date arithmetic working on days, months and years. we first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the f proof assistant. building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations. we finally integrate our approach in the catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the catala implementation of the french housing benefits, leading to the discovery of several date-related ambiguities.",2024-03-13
"146",146,"understanding and evaluating developer behaviour in programming tasks","martin schröer, rainer koschke","software engineering (cs.se)","to evaluate how developers perform differently in solving programming tasks, i.e., which actions and behaviours are more beneficial to them than others and if there are any specific strategies and behaviours that may indicate good versus poor understanding of the task and program given to them, we used the mimesis plug-in to record developers' interactions with the ide. in a series of three studies we investigated the specific behaviour of developers solving a specific programming task. we focused on which source code files they visited, how they related pieces of code and knowledge to others and when and how successful they performed code edits. to cope with the variety of behaviours due to interpersonal differences such as different level of knowledge, development style or problem solving stratiegies, we used an abstraction of the observed behaviour, which enables for a better comparison between different individual attributes such as skill, speed and used stratiegies and also facilitates later automatic evaluation of behaviours, i.e. by using a software to react to.",2024-03-13
"147",147,"user-centric beam selection and precoding design for coordinated multiple-satellite systems","vu nguyen ha, duy h. n. nguyen, juan c.-m. duncan, jorge l. gonzalez-rios, juan a. vasquez, geoffrey eappen, luis m. garces-socarras, rakesh palisetty, symeon chatzinotas, bjorn ottersten","signal processing (eess.sp)","this paper introduces a joint optimization framework for user-centric beam selection and linear precoding (lp) design in a coordinated multiple-satellite (comsat) system, employing a digital-fourier-transform-based (dft) beamforming (bf) technique. regarding serving users at their target sinrs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing lp for every satellite-to-user transmission. these technical objectives are first framed as a complex mixed-integer programming (mip) challenge. to tackle this, we reformulate it into a joint cluster association and lp design problem. then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution. additionally, a simpler duality approach for rapid beam selection and lp design is presented for comparison purposes. simulation results underscore the effectiveness of our proposed schemes across various settings.",2024-03-13
"148",148,"mastering text, code and math simultaneously via fusing highly specialized language models","ning ding, yulin chen, ganqu cui, xingtai lv, weilin zhao, ruobing xie, bowen zhou, zhiyuan liu, maosong sun","computation and language (cs.cl)","underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (llms) that strive to achieve high performance across all three domains simultaneously. achieving a very high level of proficiency for an llm within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. in this paper, we propose to fuse models that are already highly-specialized directly. the proposed fusing framework, ultrafuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. a token-level gating mechanism is introduced to blend the specialists' outputs. a two-stage training strategy accompanied by balanced sampling is designed to ensure stability. to effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, ultrachat , which includes text, code, and mathematical content. this dataset comprises approximately , instructions and covers a wide range of topics in each domain. experiments show that our model could simultaneously achieve mastery of the three crucial domains.",2024-03-13
"149",149,"bayesflo: bayesian fault localization of complex software systems","yi ji, simon mak, ryan lekivetz, joseph morgan","software engineering (cs.se)","software testing is essential for the reliable development of complex software systems. a key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. existing fault localization methods, however, are largely deterministic, and thus do not provide a principled approach for assessing probabilistic risk of potential root causes, or for integrating domain and/or structural knowledge from test engineers. to address this, we propose a novel bayesian fault localization framework called bayesflo, which leverages a flexible bayesian model on potential root cause combinations. a key feature of bayesflo is its integration of the principles of combination hierarchy and heredity, which capture the structured nature of failure-inducing combinations. a critical challenge, however, is the sheer number of potential root cause scenarios to consider, which renders the computation of posterior root cause probabilities infeasible even for small software systems. we thus develop new algorithms for efficient computation of such probabilities, leveraging recent tools from integer programming and graph representations. we then demonstrate the effectiveness of bayesflo over state-of-the-art fault localization methods, in a suite of numerical experiments and in two motivating case studies on the jmp xgboost interface.",2024-03-12
"150",150,"improving memory dependence prediction with static analysis","luke panayi, rohan gandhi, jim whittaker, vassilios chouliaras, martin berger, paul kelly","programming languages (cs.pl)","this paper explores the potential of communicating information gained by static analysis from compilers to out-of-order (ooo) machines, focusing on the memory dependence predictor (mdp). the mdp enables loads to issue without all in-flight store addresses being known, with minimal memory order violations. we use llvm to find loads with no dependencies and label them via their opcode. these labelled loads skip making lookups into the mdp, improving prediction accuracy by reducing false dependencies. we communicate this information in a minimally intrusive way, i.e.~without introducing additional hardware costs or instruction bandwidth, providing these improvements without any additional overhead in the cpu. we find that in select cases in spec, a significant number of load instructions can skip interacting with the mdp and lead to a performance gain. these results point to greater possibilities for static analysis as a source of near zero cost performance gains in future cpu designs.",2024-03-12
